{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c6e2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: empath in /Users/rachelng/opt/anaconda3/lib/python3.9/site-packages (0.89)\n",
      "Requirement already satisfied: requests in /Users/rachelng/opt/anaconda3/lib/python3.9/site-packages (from empath) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rachelng/opt/anaconda3/lib/python3.9/site-packages (from requests->empath) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rachelng/opt/anaconda3/lib/python3.9/site-packages (from requests->empath) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rachelng/opt/anaconda3/lib/python3.9/site-packages (from requests->empath) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rachelng/opt/anaconda3/lib/python3.9/site-packages (from requests->empath) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7029e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ca3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rachelng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/rachelng/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36d77d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      1  A little less than a decade ago, hockey fans w...\n",
       "1      1  The writers of the HBO series The Sopranos too...\n",
       "2      1  Despite claims from the TV news outlet to offe...\n",
       "3      1  After receiving 'subpar' service and experienc...\n",
       "4      1  After watching his beloved Seattle Mariners pr..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/fulltrain.csv\", names=[\"Label\", \"Text\"])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d9ce40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      1  When so many actors seem content to churn out ...\n",
       "1      1   In what football insiders are calling an unex...\n",
       "2      1  In a freak accident following Game 3 of the N....\n",
       "3      1  North Koreas official news agency announced to...\n",
       "4      1  The former Alaska Governor Sarah Palin would b..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"data/balancedtest.csv\", names=[\"Label\", \"Text\"])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf36e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label    0\n",
       "Text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e442567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    17870\n",
       "1    14047\n",
       "4     9995\n",
       "2     6942\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4fd25",
   "metadata": {},
   "source": [
    "### Random sampling the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87b385de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(n=10000).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7af096",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f75363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data['Text_Clean'] = data['Text'].apply(text_lower)\n",
    "    data['Text_Clean'] = data['Text_Clean'].apply(text_remove_special_characters)\n",
    "    data['Text_Clean'] = data['Text_Clean'].apply(text_remove_stopwords)\n",
    "    data['Text_Clean'] = data['Text_Clean'].apply(text_lemmatize)\n",
    "    data['Text_Tokenized'] = data['Text_Clean'].apply(text_tokenize)\n",
    "    return data\n",
    "\n",
    "def text_lemmatize(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_list = text_tokenize(text)\n",
    "    return \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_list])\n",
    "\n",
    "def text_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def text_remove_special_characters(text):\n",
    "    return re.sub('[^a-zA-Z0-9]',' ', text)\n",
    "\n",
    "def text_remove_links(text):\n",
    "    return re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def text_remove_stopwords(text):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    word_list = text_tokenize(text)\n",
    "    return \" \".join([word for word in word_list if word not in stopword_list])\n",
    "\n",
    "def text_tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def undersample_majority_class(data, y_col, y_value):\n",
    "    majority_index = data.index[data[y_col] == y_value].tolist()\n",
    "    random.seed(10)\n",
    "    random_sample = random.sample(majority_index, round(len(majority_index) * 0.5))\n",
    "    return data.drop(random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e52d0443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24561</td>\n",
       "      <td>3</td>\n",
       "      <td>Third Journalist Killed in 3 Months in Turkey ...</td>\n",
       "      <td>third journalist killed 3 month turkey suspect...</td>\n",
       "      <td>[third, journalist, killed, 3, month, turkey, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19348</td>\n",
       "      <td>2</td>\n",
       "      <td>Welfare Leech With 12 Kids Collects More In Be...</td>\n",
       "      <td>welfare leech 12 kid collect benefit make enti...</td>\n",
       "      <td>[welfare, leech, 12, kid, collect, benefit, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35758</td>\n",
       "      <td>3</td>\n",
       "      <td>Study Involving 18,000 People Confirms Acupunc...</td>\n",
       "      <td>study involving 18 000 people confirms acupunc...</td>\n",
       "      <td>[study, involving, 18, 000, people, confirms, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6252</td>\n",
       "      <td>1</td>\n",
       "      <td>When Enron founder Kenneth Lay died suddenly, ...</td>\n",
       "      <td>enron founder kenneth lay died suddenly le two...</td>\n",
       "      <td>[enron, founder, kenneth, lay, died, suddenly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9815</td>\n",
       "      <td>1</td>\n",
       "      <td>Determined to create the definitive visual doc...</td>\n",
       "      <td>determined create definitive visual document p...</td>\n",
       "      <td>[determined, create, definitive, visual, docum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Label                                               Text  \\\n",
       "0  24561      3  Third Journalist Killed in 3 Months in Turkey ...   \n",
       "1  19348      2  Welfare Leech With 12 Kids Collects More In Be...   \n",
       "2  35758      3  Study Involving 18,000 People Confirms Acupunc...   \n",
       "3   6252      1  When Enron founder Kenneth Lay died suddenly, ...   \n",
       "4   9815      1  Determined to create the definitive visual doc...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  third journalist killed 3 month turkey suspect...   \n",
       "1  welfare leech 12 kid collect benefit make enti...   \n",
       "2  study involving 18 000 people confirms acupunc...   \n",
       "3  enron founder kenneth lay died suddenly le two...   \n",
       "4  determined create definitive visual document p...   \n",
       "\n",
       "                                      Text_Tokenized  \n",
       "0  [third, journalist, killed, 3, month, turkey, ...  \n",
       "1  [welfare, leech, 12, kid, collect, benefit, ma...  \n",
       "2  [study, involving, 18, 000, people, confirms, ...  \n",
       "3  [enron, founder, kenneth, lay, died, suddenly,...  \n",
       "4  [determined, create, definitive, visual, docum...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = preprocess(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c71fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "      <td>many actor seem content churn performance quic...</td>\n",
       "      <td>[many, actor, seem, content, churn, performanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "      <td>football insider calling unexpectedly severe p...</td>\n",
       "      <td>[football, insider, calling, unexpectedly, sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "      <td>freak accident following game 3 n b final clev...</td>\n",
       "      <td>[freak, accident, following, game, 3, n, b, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "      <td>north korea official news agency announced tod...</td>\n",
       "      <td>[north, korea, official, news, agency, announc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "      <td>former alaska governor sarah palin would bring...</td>\n",
       "      <td>[former, alaska, governor, sarah, palin, would...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text  \\\n",
       "0      1  When so many actors seem content to churn out ...   \n",
       "1      1   In what football insiders are calling an unex...   \n",
       "2      1  In a freak accident following Game 3 of the N....   \n",
       "3      1  North Koreas official news agency announced to...   \n",
       "4      1  The former Alaska Governor Sarah Palin would b...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  many actor seem content churn performance quic...   \n",
       "1  football insider calling unexpectedly severe p...   \n",
       "2  freak accident following game 3 n b final clev...   \n",
       "3  north korea official news agency announced tod...   \n",
       "4  former alaska governor sarah palin would bring...   \n",
       "\n",
       "                                      Text_Tokenized  \n",
       "0  [many, actor, seem, content, churn, performanc...  \n",
       "1  [football, insider, calling, unexpectedly, sev...  \n",
       "2  [freak, accident, following, game, 3, n, b, fi...  \n",
       "3  [north, korea, official, news, agency, announc...  \n",
       "4  [former, alaska, governor, sarah, palin, would...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = preprocess(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055dbb66",
   "metadata": {},
   "source": [
    "# Baseline tf-idf NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a476e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 87515)\t0.09386281502534428\n",
      "  (0, 86720)\t0.07645718439808229\n",
      "  (0, 86627)\t0.10266157430241152\n",
      "  (0, 86443)\t0.04637225729814395\n",
      "  (0, 85140)\t0.04837848655577018\n",
      "  (0, 84942)\t0.042101212467724206\n",
      "  (0, 84921)\t0.08730989417128553\n",
      "  (0, 84809)\t0.08802435446842397\n",
      "  (0, 84438)\t0.07935565275111378\n",
      "  (0, 79712)\t0.12443038046976886\n",
      "  (0, 78959)\t0.03243337258916965\n",
      "  (0, 77945)\t0.06235335291495431\n",
      "  (0, 76648)\t0.12122960476222419\n",
      "  (0, 75874)\t0.11813194416985015\n",
      "  (0, 75086)\t0.10396767496707465\n",
      "  (0, 75051)\t0.10896831547236048\n",
      "  (0, 74293)\t0.07339833969598303\n",
      "  (0, 74103)\t0.12502299060411737\n",
      "  (0, 74054)\t0.06472906529647358\n",
      "  (0, 69048)\t0.13366648622594446\n",
      "  (0, 67293)\t0.16589552137120842\n",
      "  (0, 66821)\t0.04254406192307244\n",
      "  (0, 66175)\t0.08186379897416068\n",
      "  (0, 65214)\t0.11010224425145058\n",
      "  (0, 64418)\t0.05014427672585871\n",
      "  :\t:\n",
      "  (2999, 14668)\t0.05263574245414766\n",
      "  (2999, 14362)\t0.04897440433814833\n",
      "  (2999, 13246)\t0.04741251475123183\n",
      "  (2999, 12976)\t0.04632475641915444\n",
      "  (2999, 11259)\t0.042121231698904624\n",
      "  (2999, 10785)\t0.07516655001945156\n",
      "  (2999, 10784)\t0.08132015414264365\n",
      "  (2999, 10782)\t0.06066613742559266\n",
      "  (2999, 9583)\t0.051457711530799276\n",
      "  (2999, 9529)\t0.15160742483319778\n",
      "  (2999, 8950)\t0.2976850929188103\n",
      "  (2999, 8660)\t0.09372895806416963\n",
      "  (2999, 8090)\t0.03721163778329646\n",
      "  (2999, 8086)\t0.09847160110686498\n",
      "  (2999, 7840)\t0.08335721929900841\n",
      "  (2999, 6370)\t0.03964433868927253\n",
      "  (2999, 5331)\t0.05935705628390242\n",
      "  (2999, 4886)\t0.08465949720553274\n",
      "  (2999, 4731)\t0.04847541316498468\n",
      "  (2999, 4476)\t0.09006010516554883\n",
      "  (2999, 4301)\t0.05255853559325553\n",
      "  (2999, 3721)\t0.026161009181973175\n",
      "  (2999, 1153)\t0.049407134215506916\n",
      "  (2999, 1149)\t0.05398924477393031\n",
      "  (2999, 246)\t0.04119712074870482\n",
      "accuracy:   0.326\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.57      0.22      0.31       750\n",
      "         2 - Hoax       1.00      0.00      0.01       750\n",
      "   3 - Propaganda       0.28      1.00      0.44       750\n",
      "4 - Reliable News       1.00      0.08      0.15       750\n",
      "\n",
      "         accuracy                           0.33      3000\n",
      "        macro avg       0.71      0.33      0.23      3000\n",
      "     weighted avg       0.71      0.33      0.23      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[163   0 587   0]\n",
      " [ 27   2 721   0]\n",
      " [  0   0 750   0]\n",
      " [ 95   0 592  63]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "vectorized_train_X = vectorizer.fit_transform(train[\"Text_Clean\"])\n",
    "train_y = train[\"Label\"]\n",
    "\n",
    "vectorized_test_X = vectorizer.transform(test[\"Text_Clean\"])\n",
    "test_y = test[\"Label\"]\n",
    "\n",
    "print(vectorized_test_X)\n",
    "\n",
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(vectorized_train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(vectorized_test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c4f13",
   "metadata": {},
   "source": [
    "This will be the baseline to which we aim to improve.\n",
    "\n",
    "From the metrics calculated, we see that Reliable news is being predicted with a precision of 100%. This means that all articles with labelled \"Reliable News\" were correctly identifies as \"Reliable News\". However, articles of other lablels scored lower on the metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3f48c",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1359c07",
   "metadata": {},
   "source": [
    "#### Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae6a8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87744b93",
   "metadata": {},
   "source": [
    "#### Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cecc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af16ae",
   "metadata": {},
   "source": [
    "#### Number of Capital Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c262f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_chars(text):\n",
    "    count=0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045fab8",
   "metadata": {},
   "source": [
    "#### Number of Capital Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f648a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d76a8",
   "metadata": {},
   "source": [
    "#### Testing Syntactic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3241c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_syntactic_features(data):\n",
    "    data['Char_Count'] = data[\"Text\"].apply(count_chars)\n",
    "    data['Word_Count'] = data[\"Text\"].apply(count_words)\n",
    "    data['Capital_Chars_Count'] = data[\"Text\"].apply(count_capital_chars)\n",
    "    data['Capital_Words_Count'] = data[\"Text\"].apply(count_capital_words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "916b2465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "      <th>Char_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Capital_Chars_Count</th>\n",
       "      <th>Capital_Words_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24561</td>\n",
       "      <td>3</td>\n",
       "      <td>Third Journalist Killed in 3 Months in Turkey ...</td>\n",
       "      <td>third journalist killed 3 month turkey suspect...</td>\n",
       "      <td>[third, journalist, killed, 3, month, turkey, ...</td>\n",
       "      <td>6024</td>\n",
       "      <td>951</td>\n",
       "      <td>235</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19348</td>\n",
       "      <td>2</td>\n",
       "      <td>Welfare Leech With 12 Kids Collects More In Be...</td>\n",
       "      <td>welfare leech 12 kid collect benefit make enti...</td>\n",
       "      <td>[welfare, leech, 12, kid, collect, benefit, ma...</td>\n",
       "      <td>2408</td>\n",
       "      <td>434</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35758</td>\n",
       "      <td>3</td>\n",
       "      <td>Study Involving 18,000 People Confirms Acupunc...</td>\n",
       "      <td>study involving 18 000 people confirms acupunc...</td>\n",
       "      <td>[study, involving, 18, 000, people, confirms, ...</td>\n",
       "      <td>2706</td>\n",
       "      <td>398</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6252</td>\n",
       "      <td>1</td>\n",
       "      <td>When Enron founder Kenneth Lay died suddenly, ...</td>\n",
       "      <td>enron founder kenneth lay died suddenly le two...</td>\n",
       "      <td>[enron, founder, kenneth, lay, died, suddenly,...</td>\n",
       "      <td>1956</td>\n",
       "      <td>341</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9815</td>\n",
       "      <td>1</td>\n",
       "      <td>Determined to create the definitive visual doc...</td>\n",
       "      <td>determined create definitive visual document p...</td>\n",
       "      <td>[determined, create, definitive, visual, docum...</td>\n",
       "      <td>1064</td>\n",
       "      <td>163</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Label                                               Text  \\\n",
       "0  24561      3  Third Journalist Killed in 3 Months in Turkey ...   \n",
       "1  19348      2  Welfare Leech With 12 Kids Collects More In Be...   \n",
       "2  35758      3  Study Involving 18,000 People Confirms Acupunc...   \n",
       "3   6252      1  When Enron founder Kenneth Lay died suddenly, ...   \n",
       "4   9815      1  Determined to create the definitive visual doc...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  third journalist killed 3 month turkey suspect...   \n",
       "1  welfare leech 12 kid collect benefit make enti...   \n",
       "2  study involving 18 000 people confirms acupunc...   \n",
       "3  enron founder kenneth lay died suddenly le two...   \n",
       "4  determined create definitive visual document p...   \n",
       "\n",
       "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
       "0  [third, journalist, killed, 3, month, turkey, ...        6024         951   \n",
       "1  [welfare, leech, 12, kid, collect, benefit, ma...        2408         434   \n",
       "2  [study, involving, 18, 000, people, confirms, ...        2706         398   \n",
       "3  [enron, founder, kenneth, lay, died, suddenly,...        1956         341   \n",
       "4  [determined, create, definitive, visual, docum...        1064         163   \n",
       "\n",
       "   Capital_Chars_Count  Capital_Words_Count  \n",
       "0                  235                   22  \n",
       "1                   66                   10  \n",
       "2                   63                    1  \n",
       "3                   46                    1  \n",
       "4                   23                    2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = generate_syntactic_features(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60c33e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "      <th>Char_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Capital_Chars_Count</th>\n",
       "      <th>Capital_Words_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "      <td>many actor seem content churn performance quic...</td>\n",
       "      <td>[many, actor, seem, content, churn, performanc...</td>\n",
       "      <td>1356</td>\n",
       "      <td>251</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "      <td>football insider calling unexpectedly severe p...</td>\n",
       "      <td>[football, insider, calling, unexpectedly, sev...</td>\n",
       "      <td>1173</td>\n",
       "      <td>202</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "      <td>freak accident following game 3 n b final clev...</td>\n",
       "      <td>[freak, accident, following, game, 3, n, b, fi...</td>\n",
       "      <td>979</td>\n",
       "      <td>167</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "      <td>north korea official news agency announced tod...</td>\n",
       "      <td>[north, korea, official, news, agency, announc...</td>\n",
       "      <td>814</td>\n",
       "      <td>134</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "      <td>former alaska governor sarah palin would bring...</td>\n",
       "      <td>[former, alaska, governor, sarah, palin, would...</td>\n",
       "      <td>1120</td>\n",
       "      <td>177</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text  \\\n",
       "0      1  When so many actors seem content to churn out ...   \n",
       "1      1   In what football insiders are calling an unex...   \n",
       "2      1  In a freak accident following Game 3 of the N....   \n",
       "3      1  North Koreas official news agency announced to...   \n",
       "4      1  The former Alaska Governor Sarah Palin would b...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  many actor seem content churn performance quic...   \n",
       "1  football insider calling unexpectedly severe p...   \n",
       "2  freak accident following game 3 n b final clev...   \n",
       "3  north korea official news agency announced tod...   \n",
       "4  former alaska governor sarah palin would bring...   \n",
       "\n",
       "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
       "0  [many, actor, seem, content, churn, performanc...        1356         251   \n",
       "1  [football, insider, calling, unexpectedly, sev...        1173         202   \n",
       "2  [freak, accident, following, game, 3, n, b, fi...         979         167   \n",
       "3  [north, korea, official, news, agency, announc...         814         134   \n",
       "4  [former, alaska, governor, sarah, palin, would...        1120         177   \n",
       "\n",
       "   Capital_Chars_Count  Capital_Words_Count  \n",
       "0                   31                    4  \n",
       "1                   40                    2  \n",
       "2                   27                    1  \n",
       "3                   28                    2  \n",
       "4                   36                    4  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = generate_syntactic_features(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5795dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Char_Count\", \n",
    "            \"Word_Count\",\n",
    "            \"Capital_Chars_Count\", \n",
    "            \"Capital_Words_Count\"]\n",
    "vectorized_train_X_df = pd.DataFrame(vectorized_train_X.toarray())\n",
    "vectorized_test_X_df = pd.DataFrame(vectorized_test_X.toarray())\n",
    "\n",
    "train_X = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_X = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cac551f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.268\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.75      0.00      0.01       750\n",
      "         2 - Hoax       0.93      0.04      0.07       750\n",
      "   3 - Propaganda       0.25      1.00      0.41       750\n",
      "4 - Reliable News       0.96      0.03      0.06       750\n",
      "\n",
      "         accuracy                           0.27      3000\n",
      "        macro avg       0.72      0.27      0.14      3000\n",
      "     weighted avg       0.72      0.27      0.14      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[  3   1 745   1]\n",
      " [  0  28 722   0]\n",
      " [  0   0 750   0]\n",
      " [  1   1 725  23]]\n"
     ]
    }
   ],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c02f19",
   "metadata": {},
   "source": [
    "## Feature Engineering for Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2eed37",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00db1b",
   "metadata": {},
   "source": [
    "#### TextBlob Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfcba199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_sentiment_analysis(data):\n",
    "    data['Blob_Polarity'] = data['Text_Clean'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    data['Blob_Subjectivity'] = data['Text_Clean'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cebf3f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "      <th>Char_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Capital_Chars_Count</th>\n",
       "      <th>Capital_Words_Count</th>\n",
       "      <th>Blob_Polarity</th>\n",
       "      <th>Blob_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24561</td>\n",
       "      <td>3</td>\n",
       "      <td>Third Journalist Killed in 3 Months in Turkey ...</td>\n",
       "      <td>third journalist killed 3 month turkey suspect...</td>\n",
       "      <td>[third, journalist, killed, 3, month, turkey, ...</td>\n",
       "      <td>6024</td>\n",
       "      <td>951</td>\n",
       "      <td>235</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.031443</td>\n",
       "      <td>0.326302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19348</td>\n",
       "      <td>2</td>\n",
       "      <td>Welfare Leech With 12 Kids Collects More In Be...</td>\n",
       "      <td>welfare leech 12 kid collect benefit make enti...</td>\n",
       "      <td>[welfare, leech, 12, kid, collect, benefit, ma...</td>\n",
       "      <td>2408</td>\n",
       "      <td>434</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.408631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35758</td>\n",
       "      <td>3</td>\n",
       "      <td>Study Involving 18,000 People Confirms Acupunc...</td>\n",
       "      <td>study involving 18 000 people confirms acupunc...</td>\n",
       "      <td>[study, involving, 18, 000, people, confirms, ...</td>\n",
       "      <td>2706</td>\n",
       "      <td>398</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133428</td>\n",
       "      <td>0.319934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6252</td>\n",
       "      <td>1</td>\n",
       "      <td>When Enron founder Kenneth Lay died suddenly, ...</td>\n",
       "      <td>enron founder kenneth lay died suddenly le two...</td>\n",
       "      <td>[enron, founder, kenneth, lay, died, suddenly,...</td>\n",
       "      <td>1956</td>\n",
       "      <td>341</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.176077</td>\n",
       "      <td>0.563265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9815</td>\n",
       "      <td>1</td>\n",
       "      <td>Determined to create the definitive visual doc...</td>\n",
       "      <td>determined create definitive visual document p...</td>\n",
       "      <td>[determined, create, definitive, visual, docum...</td>\n",
       "      <td>1064</td>\n",
       "      <td>163</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.202329</td>\n",
       "      <td>0.615476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Label                                               Text  \\\n",
       "0  24561      3  Third Journalist Killed in 3 Months in Turkey ...   \n",
       "1  19348      2  Welfare Leech With 12 Kids Collects More In Be...   \n",
       "2  35758      3  Study Involving 18,000 People Confirms Acupunc...   \n",
       "3   6252      1  When Enron founder Kenneth Lay died suddenly, ...   \n",
       "4   9815      1  Determined to create the definitive visual doc...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  third journalist killed 3 month turkey suspect...   \n",
       "1  welfare leech 12 kid collect benefit make enti...   \n",
       "2  study involving 18 000 people confirms acupunc...   \n",
       "3  enron founder kenneth lay died suddenly le two...   \n",
       "4  determined create definitive visual document p...   \n",
       "\n",
       "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
       "0  [third, journalist, killed, 3, month, turkey, ...        6024         951   \n",
       "1  [welfare, leech, 12, kid, collect, benefit, ma...        2408         434   \n",
       "2  [study, involving, 18, 000, people, confirms, ...        2706         398   \n",
       "3  [enron, founder, kenneth, lay, died, suddenly,...        1956         341   \n",
       "4  [determined, create, definitive, visual, docum...        1064         163   \n",
       "\n",
       "   Capital_Chars_Count  Capital_Words_Count  Blob_Polarity  Blob_Subjectivity  \n",
       "0                  235                   22      -0.031443           0.326302  \n",
       "1                   66                   10       0.023333           0.408631  \n",
       "2                   63                    1       0.133428           0.319934  \n",
       "3                   46                    1       0.176077           0.563265  \n",
       "4                   23                    2       0.202329           0.615476  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = textblob_sentiment_analysis(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de8dc376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "      <th>Char_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Capital_Chars_Count</th>\n",
       "      <th>Capital_Words_Count</th>\n",
       "      <th>Blob_Polarity</th>\n",
       "      <th>Blob_Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "      <td>many actor seem content churn performance quic...</td>\n",
       "      <td>[many, actor, seem, content, churn, performanc...</td>\n",
       "      <td>1356</td>\n",
       "      <td>251</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0.124375</td>\n",
       "      <td>0.509644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "      <td>football insider calling unexpectedly severe p...</td>\n",
       "      <td>[football, insider, calling, unexpectedly, sev...</td>\n",
       "      <td>1173</td>\n",
       "      <td>202</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0.130303</td>\n",
       "      <td>0.543603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "      <td>freak accident following game 3 n b final clev...</td>\n",
       "      <td>[freak, accident, following, game, 3, n, b, fi...</td>\n",
       "      <td>979</td>\n",
       "      <td>167</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.426667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "      <td>north korea official news agency announced tod...</td>\n",
       "      <td>[north, korea, official, news, agency, announc...</td>\n",
       "      <td>814</td>\n",
       "      <td>134</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.375054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "      <td>former alaska governor sarah palin would bring...</td>\n",
       "      <td>[former, alaska, governor, sarah, palin, would...</td>\n",
       "      <td>1120</td>\n",
       "      <td>177</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067614</td>\n",
       "      <td>0.334870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text  \\\n",
       "0      1  When so many actors seem content to churn out ...   \n",
       "1      1   In what football insiders are calling an unex...   \n",
       "2      1  In a freak accident following Game 3 of the N....   \n",
       "3      1  North Koreas official news agency announced to...   \n",
       "4      1  The former Alaska Governor Sarah Palin would b...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  many actor seem content churn performance quic...   \n",
       "1  football insider calling unexpectedly severe p...   \n",
       "2  freak accident following game 3 n b final clev...   \n",
       "3  north korea official news agency announced tod...   \n",
       "4  former alaska governor sarah palin would bring...   \n",
       "\n",
       "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
       "0  [many, actor, seem, content, churn, performanc...        1356         251   \n",
       "1  [football, insider, calling, unexpectedly, sev...        1173         202   \n",
       "2  [freak, accident, following, game, 3, n, b, fi...         979         167   \n",
       "3  [north, korea, official, news, agency, announc...         814         134   \n",
       "4  [former, alaska, governor, sarah, palin, would...        1120         177   \n",
       "\n",
       "   Capital_Chars_Count  Capital_Words_Count  Blob_Polarity  Blob_Subjectivity  \n",
       "0                   31                    4       0.124375           0.509644  \n",
       "1                   40                    2       0.130303           0.543603  \n",
       "2                   27                    1      -0.062500           0.426667  \n",
       "3                   28                    2       0.003409           0.375054  \n",
       "4                   36                    4       0.067614           0.334870  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = textblob_sentiment_analysis(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b8e6b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = [\"Blob_Polarity\", \n",
    "            \"Blob_Subjectivity\",]\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d75ab61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    }
   ],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf37c24",
   "metadata": {},
   "source": [
    "#### Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "810cfa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_analysis(data):\n",
    "    data['Vader_Scores'] = data['Text_Clean'].apply(lambda x: vader.polarity_scores(x))\n",
    "    data['Vader_Negative'] = data['Vader_Scores'].apply(lambda x: x['neg'])\n",
    "    data['Vader_Neutral'] = data['Vader_Scores'].apply(lambda x: x['neu'])\n",
    "    data['Vader_Positive'] = data['Vader_Scores'].apply(lambda x: x['pos'])\n",
    "    data['Vader_Compound'] = data['Vader_Scores'].apply(lambda x: x['compound'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "682e2a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "      <th>Char_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Capital_Chars_Count</th>\n",
       "      <th>Capital_Words_Count</th>\n",
       "      <th>Blob_Polarity</th>\n",
       "      <th>Blob_Subjectivity</th>\n",
       "      <th>Vader_Scores</th>\n",
       "      <th>Vader_Negative</th>\n",
       "      <th>Vader_Neutral</th>\n",
       "      <th>Vader_Positive</th>\n",
       "      <th>Vader_Compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24561</td>\n",
       "      <td>3</td>\n",
       "      <td>Third Journalist Killed in 3 Months in Turkey ...</td>\n",
       "      <td>third journalist killed 3 month turkey suspect...</td>\n",
       "      <td>[third, journalist, killed, 3, month, turkey, ...</td>\n",
       "      <td>6024</td>\n",
       "      <td>951</td>\n",
       "      <td>235</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.031443</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>{'neg': 0.277, 'neu': 0.636, 'pos': 0.087, 'co...</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19348</td>\n",
       "      <td>2</td>\n",
       "      <td>Welfare Leech With 12 Kids Collects More In Be...</td>\n",
       "      <td>welfare leech 12 kid collect benefit make enti...</td>\n",
       "      <td>[welfare, leech, 12, kid, collect, benefit, ma...</td>\n",
       "      <td>2408</td>\n",
       "      <td>434</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.408631</td>\n",
       "      <td>{'neg': 0.153, 'neu': 0.616, 'pos': 0.231, 'co...</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.9573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35758</td>\n",
       "      <td>3</td>\n",
       "      <td>Study Involving 18,000 People Confirms Acupunc...</td>\n",
       "      <td>study involving 18 000 people confirms acupunc...</td>\n",
       "      <td>[study, involving, 18, 000, people, confirms, ...</td>\n",
       "      <td>2706</td>\n",
       "      <td>398</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133428</td>\n",
       "      <td>0.319934</td>\n",
       "      <td>{'neg': 0.162, 'neu': 0.725, 'pos': 0.113, 'co...</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.9509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6252</td>\n",
       "      <td>1</td>\n",
       "      <td>When Enron founder Kenneth Lay died suddenly, ...</td>\n",
       "      <td>enron founder kenneth lay died suddenly le two...</td>\n",
       "      <td>[enron, founder, kenneth, lay, died, suddenly,...</td>\n",
       "      <td>1956</td>\n",
       "      <td>341</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.176077</td>\n",
       "      <td>0.563265</td>\n",
       "      <td>{'neg': 0.097, 'neu': 0.63, 'pos': 0.274, 'com...</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.9903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9815</td>\n",
       "      <td>1</td>\n",
       "      <td>Determined to create the definitive visual doc...</td>\n",
       "      <td>determined create definitive visual document p...</td>\n",
       "      <td>[determined, create, definitive, visual, docum...</td>\n",
       "      <td>1064</td>\n",
       "      <td>163</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.202329</td>\n",
       "      <td>0.615476</td>\n",
       "      <td>{'neg': 0.116, 'neu': 0.57, 'pos': 0.314, 'com...</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.9788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Label                                               Text  \\\n",
       "0  24561      3  Third Journalist Killed in 3 Months in Turkey ...   \n",
       "1  19348      2  Welfare Leech With 12 Kids Collects More In Be...   \n",
       "2  35758      3  Study Involving 18,000 People Confirms Acupunc...   \n",
       "3   6252      1  When Enron founder Kenneth Lay died suddenly, ...   \n",
       "4   9815      1  Determined to create the definitive visual doc...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  third journalist killed 3 month turkey suspect...   \n",
       "1  welfare leech 12 kid collect benefit make enti...   \n",
       "2  study involving 18 000 people confirms acupunc...   \n",
       "3  enron founder kenneth lay died suddenly le two...   \n",
       "4  determined create definitive visual document p...   \n",
       "\n",
       "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
       "0  [third, journalist, killed, 3, month, turkey, ...        6024         951   \n",
       "1  [welfare, leech, 12, kid, collect, benefit, ma...        2408         434   \n",
       "2  [study, involving, 18, 000, people, confirms, ...        2706         398   \n",
       "3  [enron, founder, kenneth, lay, died, suddenly,...        1956         341   \n",
       "4  [determined, create, definitive, visual, docum...        1064         163   \n",
       "\n",
       "   Capital_Chars_Count  Capital_Words_Count  Blob_Polarity  Blob_Subjectivity  \\\n",
       "0                  235                   22      -0.031443           0.326302   \n",
       "1                   66                   10       0.023333           0.408631   \n",
       "2                   63                    1       0.133428           0.319934   \n",
       "3                   46                    1       0.176077           0.563265   \n",
       "4                   23                    2       0.202329           0.615476   \n",
       "\n",
       "                                        Vader_Scores  Vader_Negative  \\\n",
       "0  {'neg': 0.277, 'neu': 0.636, 'pos': 0.087, 'co...           0.277   \n",
       "1  {'neg': 0.153, 'neu': 0.616, 'pos': 0.231, 'co...           0.153   \n",
       "2  {'neg': 0.162, 'neu': 0.725, 'pos': 0.113, 'co...           0.162   \n",
       "3  {'neg': 0.097, 'neu': 0.63, 'pos': 0.274, 'com...           0.097   \n",
       "4  {'neg': 0.116, 'neu': 0.57, 'pos': 0.314, 'com...           0.116   \n",
       "\n",
       "   Vader_Neutral  Vader_Positive  Vader_Compound  \n",
       "0          0.636           0.087         -0.9994  \n",
       "1          0.616           0.231          0.9573  \n",
       "2          0.725           0.113         -0.9509  \n",
       "3          0.630           0.274          0.9903  \n",
       "4          0.570           0.314          0.9788  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = vader_sentiment_analysis(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bb7a714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "      <th>Char_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Capital_Chars_Count</th>\n",
       "      <th>Capital_Words_Count</th>\n",
       "      <th>Blob_Polarity</th>\n",
       "      <th>Blob_Subjectivity</th>\n",
       "      <th>Vader_Scores</th>\n",
       "      <th>Vader_Negative</th>\n",
       "      <th>Vader_Neutral</th>\n",
       "      <th>Vader_Positive</th>\n",
       "      <th>Vader_Compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "      <td>many actor seem content churn performance quic...</td>\n",
       "      <td>[many, actor, seem, content, churn, performanc...</td>\n",
       "      <td>1356</td>\n",
       "      <td>251</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0.124375</td>\n",
       "      <td>0.509644</td>\n",
       "      <td>{'neg': 0.083, 'neu': 0.728, 'pos': 0.189, 'co...</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.9131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "      <td>football insider calling unexpectedly severe p...</td>\n",
       "      <td>[football, insider, calling, unexpectedly, sev...</td>\n",
       "      <td>1173</td>\n",
       "      <td>202</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0.130303</td>\n",
       "      <td>0.543603</td>\n",
       "      <td>{'neg': 0.256, 'neu': 0.665, 'pos': 0.079, 'co...</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.079</td>\n",
       "      <td>-0.9751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "      <td>freak accident following game 3 n b final clev...</td>\n",
       "      <td>[freak, accident, following, game, 3, n, b, fi...</td>\n",
       "      <td>979</td>\n",
       "      <td>167</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>{'neg': 0.152, 'neu': 0.684, 'pos': 0.164, 'co...</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "      <td>north korea official news agency announced tod...</td>\n",
       "      <td>[north, korea, official, news, agency, announc...</td>\n",
       "      <td>814</td>\n",
       "      <td>134</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.375054</td>\n",
       "      <td>{'neg': 0.149, 'neu': 0.688, 'pos': 0.164, 'co...</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "      <td>former alaska governor sarah palin would bring...</td>\n",
       "      <td>[former, alaska, governor, sarah, palin, would...</td>\n",
       "      <td>1120</td>\n",
       "      <td>177</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067614</td>\n",
       "      <td>0.334870</td>\n",
       "      <td>{'neg': 0.082, 'neu': 0.782, 'pos': 0.136, 'co...</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.7579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text  \\\n",
       "0      1  When so many actors seem content to churn out ...   \n",
       "1      1   In what football insiders are calling an unex...   \n",
       "2      1  In a freak accident following Game 3 of the N....   \n",
       "3      1  North Koreas official news agency announced to...   \n",
       "4      1  The former Alaska Governor Sarah Palin would b...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  many actor seem content churn performance quic...   \n",
       "1  football insider calling unexpectedly severe p...   \n",
       "2  freak accident following game 3 n b final clev...   \n",
       "3  north korea official news agency announced tod...   \n",
       "4  former alaska governor sarah palin would bring...   \n",
       "\n",
       "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
       "0  [many, actor, seem, content, churn, performanc...        1356         251   \n",
       "1  [football, insider, calling, unexpectedly, sev...        1173         202   \n",
       "2  [freak, accident, following, game, 3, n, b, fi...         979         167   \n",
       "3  [north, korea, official, news, agency, announc...         814         134   \n",
       "4  [former, alaska, governor, sarah, palin, would...        1120         177   \n",
       "\n",
       "   Capital_Chars_Count  Capital_Words_Count  Blob_Polarity  Blob_Subjectivity  \\\n",
       "0                   31                    4       0.124375           0.509644   \n",
       "1                   40                    2       0.130303           0.543603   \n",
       "2                   27                    1      -0.062500           0.426667   \n",
       "3                   28                    2       0.003409           0.375054   \n",
       "4                   36                    4       0.067614           0.334870   \n",
       "\n",
       "                                        Vader_Scores  Vader_Negative  \\\n",
       "0  {'neg': 0.083, 'neu': 0.728, 'pos': 0.189, 'co...           0.083   \n",
       "1  {'neg': 0.256, 'neu': 0.665, 'pos': 0.079, 'co...           0.256   \n",
       "2  {'neg': 0.152, 'neu': 0.684, 'pos': 0.164, 'co...           0.152   \n",
       "3  {'neg': 0.149, 'neu': 0.688, 'pos': 0.164, 'co...           0.149   \n",
       "4  {'neg': 0.082, 'neu': 0.782, 'pos': 0.136, 'co...           0.082   \n",
       "\n",
       "   Vader_Neutral  Vader_Positive  Vader_Compound  \n",
       "0          0.728           0.189          0.9131  \n",
       "1          0.665           0.079         -0.9751  \n",
       "2          0.684           0.164          0.1280  \n",
       "3          0.688           0.164          0.1280  \n",
       "4          0.782           0.136          0.7579  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = vader_sentiment_analysis(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8306b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = [\"Vader_Negative\", \n",
    "            \"Vader_Neutral\",\n",
    "            \"Vader_Positive\",\n",
    "            \"Vader_Compound\"]\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8edf4e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.558\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[474  24 243   9]\n",
      " [117  43 582   8]\n",
      " [ 19   0 728   3]\n",
      " [141   0 181 428]]\n"
     ]
    }
   ],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeabb56",
   "metadata": {},
   "source": [
    "#### Combining Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51f77a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = [\"Blob_Polarity\",\n",
    "            \"Blob_Subjectivity\",\n",
    "            \"Vader_Negative\", \n",
    "            \"Vader_Neutral\",\n",
    "            \"Vader_Positive\",\n",
    "            \"Vader_Compound\"]\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e761a295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.558\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.64      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[477  24 241   8]\n",
      " [119  43 580   8]\n",
      " [ 19   0 728   3]\n",
      " [144   0 179 427]]\n"
     ]
    }
   ],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df47dd7",
   "metadata": {},
   "source": [
    "### Context Incongruity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0b7e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_gram(tokenized,ngram=1):\n",
    "    temp = zip(*[tokenized[i:] for i in range(0,ngram)])\n",
    "    ans = [' '.join(ngram) for ngram in temp]\n",
    "    return ans\n",
    "\n",
    "def get_N_gram_polarities(n_gram):\n",
    "    return list(map(lambda x: vader.polarity_scores(x)[\"compound\"], n_gram))\n",
    "    \n",
    "def count_context_incongruities(tokenized, N):\n",
    "    n_grams = generate_N_gram(tokenized, ngram=N)\n",
    "    n_gram_polarities = get_N_gram_polarities(n_grams)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(n_gram_polarities) - 1):\n",
    "        if n_gram_polarities[i] * n_gram_polarities[i+1] < 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4092a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_incongruities(data, N):\n",
    "    data[\"Context_Incongruity - \" + str(N) + \"-gram\"] = data[\"Text_Tokenized\"].apply(lambda x: count_context_incongruities(x, N))\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f99a50dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  Label                                               Text  \\\n",
      "0  24561      3  Third Journalist Killed in 3 Months in Turkey ...   \n",
      "1  19348      2  Welfare Leech With 12 Kids Collects More In Be...   \n",
      "2  35758      3  Study Involving 18,000 People Confirms Acupunc...   \n",
      "3   6252      1  When Enron founder Kenneth Lay died suddenly, ...   \n",
      "4   9815      1  Determined to create the definitive visual doc...   \n",
      "\n",
      "                                          Text_Clean  \\\n",
      "0  third journalist killed 3 month turkey suspect...   \n",
      "1  welfare leech 12 kid collect benefit make enti...   \n",
      "2  study involving 18 000 people confirms acupunc...   \n",
      "3  enron founder kenneth lay died suddenly le two...   \n",
      "4  determined create definitive visual document p...   \n",
      "\n",
      "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
      "0  [third, journalist, killed, 3, month, turkey, ...        6024         951   \n",
      "1  [welfare, leech, 12, kid, collect, benefit, ma...        2408         434   \n",
      "2  [study, involving, 18, 000, people, confirms, ...        2706         398   \n",
      "3  [enron, founder, kenneth, lay, died, suddenly,...        1956         341   \n",
      "4  [determined, create, definitive, visual, docum...        1064         163   \n",
      "\n",
      "   Capital_Chars_Count  Capital_Words_Count  Blob_Polarity  ...  \\\n",
      "0                  235                   22      -0.031443  ...   \n",
      "1                   66                   10       0.023333  ...   \n",
      "2                   63                    1       0.133428  ...   \n",
      "3                   46                    1       0.176077  ...   \n",
      "4                   23                    2       0.202329  ...   \n",
      "\n",
      "                                        Vader_Scores Vader_Negative  \\\n",
      "0  {'neg': 0.277, 'neu': 0.636, 'pos': 0.087, 'co...          0.277   \n",
      "1  {'neg': 0.153, 'neu': 0.616, 'pos': 0.231, 'co...          0.153   \n",
      "2  {'neg': 0.162, 'neu': 0.725, 'pos': 0.113, 'co...          0.162   \n",
      "3  {'neg': 0.097, 'neu': 0.63, 'pos': 0.274, 'com...          0.097   \n",
      "4  {'neg': 0.116, 'neu': 0.57, 'pos': 0.314, 'com...          0.116   \n",
      "\n",
      "   Vader_Neutral  Vader_Positive  Vader_Compound  \\\n",
      "0          0.636           0.087         -0.9994   \n",
      "1          0.616           0.231          0.9573   \n",
      "2          0.725           0.113         -0.9509   \n",
      "3          0.630           0.274          0.9903   \n",
      "4          0.570           0.314          0.9788   \n",
      "\n",
      "   Context_Incongruity - 1-gram  Context_Incongruity - 2-gram  \\\n",
      "0                             4                             8   \n",
      "1                             2                             3   \n",
      "2                             3                             3   \n",
      "3                             4                             3   \n",
      "4                             0                             4   \n",
      "\n",
      "   Context_Incongruity - 3-gram  Context_Incongruity - 4-gram  \\\n",
      "0                            18                            18   \n",
      "1                             6                             8   \n",
      "2                             8                             8   \n",
      "3                             6                             6   \n",
      "4                             5                             5   \n",
      "\n",
      "   Context_Incongruity - 5-gram  \n",
      "0                            20  \n",
      "1                             8  \n",
      "2                             6  \n",
      "3                             7  \n",
      "4                             5  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "   Label                                               Text  \\\n",
      "0      1  When so many actors seem content to churn out ...   \n",
      "1      1   In what football insiders are calling an unex...   \n",
      "2      1  In a freak accident following Game 3 of the N....   \n",
      "3      1  North Koreas official news agency announced to...   \n",
      "4      1  The former Alaska Governor Sarah Palin would b...   \n",
      "\n",
      "                                          Text_Clean  \\\n",
      "0  many actor seem content churn performance quic...   \n",
      "1  football insider calling unexpectedly severe p...   \n",
      "2  freak accident following game 3 n b final clev...   \n",
      "3  north korea official news agency announced tod...   \n",
      "4  former alaska governor sarah palin would bring...   \n",
      "\n",
      "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
      "0  [many, actor, seem, content, churn, performanc...        1356         251   \n",
      "1  [football, insider, calling, unexpectedly, sev...        1173         202   \n",
      "2  [freak, accident, following, game, 3, n, b, fi...         979         167   \n",
      "3  [north, korea, official, news, agency, announc...         814         134   \n",
      "4  [former, alaska, governor, sarah, palin, would...        1120         177   \n",
      "\n",
      "   Capital_Chars_Count  Capital_Words_Count  Blob_Polarity  Blob_Subjectivity  \\\n",
      "0                   31                    4       0.124375           0.509644   \n",
      "1                   40                    2       0.130303           0.543603   \n",
      "2                   27                    1      -0.062500           0.426667   \n",
      "3                   28                    2       0.003409           0.375054   \n",
      "4                   36                    4       0.067614           0.334870   \n",
      "\n",
      "                                        Vader_Scores  Vader_Negative  \\\n",
      "0  {'neg': 0.083, 'neu': 0.728, 'pos': 0.189, 'co...           0.083   \n",
      "1  {'neg': 0.256, 'neu': 0.665, 'pos': 0.079, 'co...           0.256   \n",
      "2  {'neg': 0.152, 'neu': 0.684, 'pos': 0.164, 'co...           0.152   \n",
      "3  {'neg': 0.149, 'neu': 0.688, 'pos': 0.164, 'co...           0.149   \n",
      "4  {'neg': 0.082, 'neu': 0.782, 'pos': 0.136, 'co...           0.082   \n",
      "\n",
      "   Vader_Neutral  Vader_Positive  Vader_Compound  \\\n",
      "0          0.728           0.189          0.9131   \n",
      "1          0.665           0.079         -0.9751   \n",
      "2          0.684           0.164          0.1280   \n",
      "3          0.688           0.164          0.1280   \n",
      "4          0.782           0.136          0.7579   \n",
      "\n",
      "   Context_Incongruity - 1-gram  Context_Incongruity - 2-gram  \\\n",
      "0                             0                             3   \n",
      "1                             1                             2   \n",
      "2                             0                             1   \n",
      "3                             0                             1   \n",
      "4                             1                             2   \n",
      "\n",
      "   Context_Incongruity - 3-gram  Context_Incongruity - 4-gram  \\\n",
      "0                             4                             5   \n",
      "1                             2                             6   \n",
      "2                             1                             2   \n",
      "3                             2                             4   \n",
      "4                             2                             2   \n",
      "\n",
      "   Context_Incongruity - 5-gram  \n",
      "0                             5  \n",
      "1                             6  \n",
      "2                             2  \n",
      "3                             5  \n",
      "4                             2  \n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    get_context_incongruities(train, i)\n",
    "    get_context_incongruities(test, i)\n",
    "    \n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1e9d97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = [\"Context_Incongruity - 1-gram\", \n",
    "            \"Context_Incongruity - 2-gram\",\n",
    "            \"Context_Incongruity - 3-gram\",\n",
    "            \"Context_Incongruity - 4-gram\",\n",
    "            \"Context_Incongruity - 5-gram\"]\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e374c49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.556\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.62      0.63       750\n",
      "         2 - Hoax       0.63      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.95      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[466  26 248  10]\n",
      " [113  45 583   9]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    }
   ],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2d0f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob_Polarity\n",
      "accuracy:   0.556\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.62      0.63       750\n",
      "         2 - Hoax       0.63      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.95      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[466  26 248  10]\n",
      " [113  44 584   9]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob_Subjectivity\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.62      0.63       750\n",
      "         2 - Hoax       0.63      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.95      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[468  26 246  10]\n",
      " [113  45 583   9]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader_Negative\n",
      "accuracy:   0.555\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.62      0.63       750\n",
      "         2 - Hoax       0.63      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.58       750\n",
      "4 - Reliable News       0.95      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[465  26 249  10]\n",
      " [112  45 584   9]\n",
      " [ 18   0 729   3]\n",
      " [142   0 181 427]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader_Neutral\n",
      "accuracy:   0.556\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.62      0.63       750\n",
      "         2 - Hoax       0.63      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.95      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[467  26 247  10]\n",
      " [114  44 583   9]\n",
      " [ 18   0 729   3]\n",
      " [142   0 179 429]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader_Positive\n",
      "accuracy:   0.556\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.62      0.63       750\n",
      "         2 - Hoax       0.63      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.95      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[465  26 249  10]\n",
      " [112  45 584   9]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader_Compound\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.63      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.72       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [117  43 582   8]\n",
      " [ 19   0 728   3]\n",
      " [140   0 181 429]]\n"
     ]
    }
   ],
   "source": [
    "for feature in features:\n",
    "    train_features = pd.concat([vectorized_train_X_df, train[feature]], axis=\"columns\")\n",
    "    test_features = pd.concat([vectorized_test_X_df, test[feature]], axis=\"columns\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_X = scaler.fit_transform(train_features)\n",
    "    test_X = scaler.fit_transform(test_features)\n",
    "    \n",
    "    nb_classifer = MultinomialNB()\n",
    "    nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "    pred_y = nb_classifer.predict(test_X)\n",
    "    accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "    \n",
    "    print(feature)\n",
    "    print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "    print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b888f5d",
   "metadata": {},
   "source": [
    "### Topic Modeling and Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c96084",
   "metadata": {},
   "source": [
    "#### Lexical Categories Analysis using Empath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff242aa",
   "metadata": {},
   "source": [
    "An example of what the code below is executing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db810f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"sarcastic\", \"petulant\", \"humorless\", \"boastful\", \"glib\", \"sanctimonious\", \"flippant\", \"whiny\", \"mischievous\", \"snide\", \"impetuous\", \"pompous\", \"shrill\", \"inarticulate\", \"manipulative\", \"childish\", \"imperious\", \"condescending\", \"preachy\", \"pushy\", \"excitable\", \"argumentative\", \"cantankerous\", \"overbearing\", \"haughty\", \"spiteful\", \"egotistical\", \"gruff\", \"boorish\", \"diffident\", \"meek\", \"impulsive\", \"acerbic\", \"brusque\", \"affectionate\", \"caustic\", \"flirtatious\", \"bombastic\", \"accusatory\", \"obnoxious\", \"laconic\", \"sardonic\", \"wry\", \"curt\", \"sly\", \"overwrought\", \"devious\", \"endearingly\", \"goofy\", \"introspective\", \"moody\", \"easygoing\", \"ingenuous\", \"pedantic\", \"morose\", \"brash\", \"mawkish\", \"humorous\", \"empathetic\", \"surly\", \"feckless\", \"dutiful\", \"chatty\", \"bemused\", \"droll\", \"irascible\", \"childlike\", \"deadpan\", \"witless\", \"judgmental\", \"unlikable\", \"callow\", \"temperamental\", \"jocular\", \"incoherent\", \"folksy\", \"neurotic\", \"endearing\", \"comical\", \"verbose\", \"sneaky\", \"abrasive\", \"narcissistic\", \"ingratiating\", \"inane\", \"dopey\", \"irreverent\", \"bookish\", \"wacky\", \"streetwise\", \"flighty\", \"ornery\", \"clumsy\", \"loquacious\", \"sentimental\", \"dour\", \"girlish\", \"sullen\"]\n",
      "[\"ironic\", \"patronizing\", \"paradoxical\", \"perverse\", \"preposterous\", \"shocking\", \"naive\", \"insulting\", \"scandalous\", \"simplistic\", \"absurd\", \"liberating\", \"laughable\", \"amusing\", \"vulgar\", \"pathetic\", \"hokey\", \"na\\u00efve\", \"puzzling\", \"pretentious\", \"trite\", \"cynical\", \"implausible\", \"myopic\", \"incomprehensible\", \"outlandish\", \"reactionary\", \"condescending\", \"audacious\", \"strange\", \"superficial\", \"anachronistic\", \"contrived\", \"didactic\", \"sentimental\", \"facile\", \"peculiar\", \"ludicrous\", \"disturbing\", \"provocative\", \"corny\", \"depressing\", \"sexist\", \"exploitative\", \"presumptuous\", \"unsettling\", \"subversive\", \"distressing\", \"disheartening\", \"grotesque\", \"comical\", \"wrongheaded\", \"appalling\", \"heartbreaking\", \"enlightening\", \"prophetic\", \"enlightened\", \"distasteful\", \"instructive\", \"disingenuous\", \"preachy\", \"inspiring\", \"bizarre\", \"disconcerting\", \"disagreeable\", \"glib\", \"childish\", \"inexplicable\", \"admirable\", \"smug\", \"prescient\", \"uplifting\", \"demeaning\", \"strident\", \"moving\", \"shameful\", \"silly\", \"banal\", \"flattering\", \"dogmatic\", \"condescending\", \"gratifying\", \"unfortunate\", \"crass\", \"humorous\", \"elitist\", \"obtuse\", \"quixotic\", \"perceptive\", \"moralistic\", \"judgmental\", \"hurtful\", \"hyperbolic\", \"hypocritical\", \"callous\", \"maddening\", \"odious\", \"nonsensical\", \"baffling\", \"muddled\"]\n",
      "[\"contradict\", \"refute\", \"contradicting\", \"contradicted\", \"rebut\", \"disavow\", \"contradicts\", \"corroborate\", \"buttress\", \"refuted\", \"disprove\", \"contrary\", \"substantiate\", \"misinterpreted\", \"imply\", \"dispute\", \"reiterate\", \"misread\", \"misconstrued\", \"concur\", \"repudiate\", \"infer\", \"erroneous\", \"vindicate\", \"state\", \"conclusive\", \"contradictory\", \"disproved\", \"validate\", \"unambiguous\", \"disregard\", \"hint\", \"confirm\", \"misrepresent\", \"obscure\", \"corroborated\", \"hearsay\", \"exaggerate\", \"conflicted\", \"unsupported\", \"prejudge\", \"clarify\", \"implicate\", \"conflict\", \"disputing\", \"underline\", \"implicit\", \"refuting\", \"rebutted\", \"inferred\", \"affirm\", \"suggest\", \"such_statements\", \"public_statements\", \"disregarded\", \"mislead\", \"construe\", \"distort\", \"prejudice\", \"exonerate\", \"exaggerated\", \"implied\", \"inferences\", \"characterizing\", \"equivocal\", \"interpret\", \"disavowed\", \"ambiguous\", \"countenance\", \"corroborates\", \"refutes\", \"taint\", \"admissible\", \"unequivocal\", \"disparage\", \"elaborated\", \"summarize\"]\n",
      "[\"mock\", \"ridicule\", \"cloak\", \"flaunt\", \"scorn\", \"disparage\", \"sneer\", \"belittle\", \"denigrate\", \"betray\", \"flatter\", \"skewer\", \"disdain\", \"pander\", \"mocking\", \"mocks\", \"demean\", \"deride\", \"glorify\", \"eschew\", \"terrify\", \"idealize\", \"detest\", \"insinuate\", \"despise\", \"recoil\", \"satirize\", \"trivialize\", \"insult\", \"spout\", \"utter\", \"exalt\", \"rebel\", \"titillate\", \"chide\", \"demonize\", \"brand\", \"lampoon\", \"romanticize\", \"wink\", \"skewering\", \"revile\", \"caricature\", \"caricatured\", \"toy\", \"confound\", \"equate\", \"hypocrites\", \"loathe\", \"confuse\", \"decry\", \"shock\", \"charm\", \"insults\", \"thrill\", \"imbue\", \"platitudes\", \"scorning\", \"displease\", \"offends\", \"distrust\", \"startle\", \"abhor\", \"flirt\", \"offend\", \"condescending\", \"contemptuous\", \"cringe\", \"enlighten\", \"revel\", \"self-righteousness\", \"castigate\", \"denigrating\", \"vilify\", \"amuse\", \"kowtow\", \"flaunting\", \"proclaim\", \"bash\", \"flattering\", \"utter\", \"condescend\", \"shameless\", \"espouse\", \"misunderstand\", \"hectoring\", \"repress\", \"exalting\", \"revere\", \"shame\", \"humiliate\", \"extol\", \"bigots\", \"wallow\", \"pontificate\", \"scornful\", \"mocked\", \"pandered\", \"torment\", \"coddle\"]\n",
      "[\"jest\", \"jokingly\", \"facetiously\", \"joking\", \"heartedly\", \"apologetically\", \"half\", \"ruefully\", \"joke\"]\n",
      "[\"malicious\", \"despicable\", \"deceitful\", \"willful\", \"hateful\", \"vindictive\", \"vile\", \"childish\", \"reprehensible\", \"gratuitously\", \"cowardly\", \"demeaning\", \"dishonest\", \"blatant\", \"sexist\", \"manipulative\", \"unprofessional\", \"duplicitous\", \"insinuation\", \"devious\", \"inflammatory\", \"reckless\", \"repulsive\", \"callous\", \"contemptible\", \"unintentional\", \"gratuitous\", \"vicious\", \"patently\", \"degrading\", \"boorish\", \"scurrilous\", \"idiotic\", \"unforgivable\", \"phony\", \"insulting\", \"venal\", \"unprincipled\", \"premeditated\", \"exploitive\", \"petty\", \"pathological\", \"incoherent\", \"amoral\", \"hurtful\", \"illogical\", \"spiteful\", \"careless\", \"thoughtless\", \"disagreeable\", \"blatantly\", \"delusional\", \"heartless\", \"vulgar\", \"exploitative\", \"cruel\", \"lying\", \"inhuman\", \"vengeful\", \"poor_taste\", \"intemperate\", \"disgraceful\", \"wrongheaded\", \"bigoted\", \"condescending\", \"defamatory\", \"evasive\", \"depraved\", \"unfeeling\", \"antisocial\", \"willfully\", \"disgusting\", \"loathsome\", \"mindless\", \"disrespectful\", \"irrational\", \"sadistic\", \"shameless\", \"scandalous\", \"patronizing\", \"perverse\", \"misogynistic\", \"capricious\", \"comical\", \"transparently\", \"frivolous\", \"obnoxious\", \"sarcastic\", \"bogus\", \"untrustworthy\", \"glib\", \"ludicrous\", \"deliberate\", \"paranoid\", \"uncalled\", \"flagrant\", \"tendentious\", \"repugnant\", \"sneaky\"]\n",
      "[]\n",
      "[\"government\", \"Government\", \"Government\", \"central_government\", \"national_government\", \"central_Government\", \"local_government\", \"governments\", \"political_system\", \"American_government\", \"national_Government\", \"judiciary\", \"federal_Government\", \"foreign_governments\", \"state_government\", \"private_sector\", \"administration\", \"military\", \"Western_governments\", \"other_governments\", \"private_enterprise\", \"democratic_process\", \"federal_government\", \"Mexican_Government\", \"political_opposition\", \"European_governments\", \"executive_branch\", \"new_Government\", \"political_parties\", \"citizens\", \"the_Federal_Government\", \"Government_policy\", \"Russian_government\", \"reform\", \"free_market\", \"bureaucrats\", \"central_authority\", \"Bush_administration\", \"ruling_party\", \"Federal_bureaucracy\", \"the_United_States_Government\", \"political_influence\", \"state_control\", \"national_governments\", \"political_process\", \"policies\", \"Russian_Government\", \"democratic_system\", \"political_leadership\", \"democratic_institutions\", \"public_services\", \"own_government\", \"the_Clinton_Administration\", \"state\", \"electoral_process\", \"state_governments\", \"ministries\", \"judicial_system\", \"state_enterprises\", \"national_policy\", \"armed_forces\", \"South_African_Government\", \"economic_policies\", \"privatization\", \"Chinese_government\", \"justice_system\", \"country\", \"trade_unions\", \"electoral_system\", \"Soviet_Government\", \"reformers\", \"private_interests\", \"citizenry\", \"white_minority\", \"enterprises\", \"bureaucracy\", \"market_forces\", \"land_reform\", \"government_control\", \"the_American_Government\", \"international_organizations\", \"Western_countries\", \"reforms\", \"private_business\", \"Congress\", \"national_interest\", \"political_pressures\", \"Japanese_Government\", \"political_leaders\", \"private_industry\", \"democracy\", \"vested_interests\", \"political_activity\", \"local_governments\", \"Mexican_government\", \"the_U.S._Government\", \"public_sector\", \"foreign_aid\", \"economic_reform\", \"legal_system\"]\n",
      "[\"politics\", \"ideology\", \"local_politics\", \"public_life\", \"national_politics\", \"American_politics\", \"political_arena\", \"electoral_politics\", \"party_politics\", \"political_life\", \"partisan_politics\", \"political_culture\", \"Presidential_politics\", \"elites\", \"political_discourse\", \"public_discourse\", \"religion\", \"political_issues\", \"political_philosophy\", \"political_process\", \"New_York_politics\", \"politicking\", \"partisanship\", \"social_issues\", \"foreign_policy\", \"rhetoric\", \"foreign_affairs\", \"political_agenda\", \"academia\", \"liberalism\", \"economic_issues\", \"populism\", \"political_debate\", \"political_world\", \"political_campaign\", \"political_system\", \"domestic_politics\", \"ideologies\", \"morality\", \"state_politics\", \"public_debate\", \"political\", \"Japanese_politics\", \"culture\", \"politician\", \"demagoguery\", \"private_life\", \"discourse\", \"American_life\", \"conservatism\", \"political_power\", \"journalism\", \"world_affairs\", \"ideological\", \"racial_politics\", \"Washington_politics\", \"class_warfare\", \"activism\", \"religious_right\", \"moral_values\", \"personal_destruction\", \"pragmatism\", \"domestic_policy\", \"high_office\", \"culture_wars\", \"debates\", \"political_correctness\", \"politicians\", \"social_policy\", \"domestic_issues\", \"philosophy\", \"orthodoxy\", \"moral_issues\", \"ideologues\", \"racial_issues\", \"multiculturalism\", \"conservative_movement\", \"political_scene\", \"American_society\", \"family_values\", \"modern_politics\", \"presidential_politics\", \"American_foreign_policy\", \"public_policy\", \"dogma\", \"public_service\", \"feminism\", \"domestic_affairs\", \"patronage\", \"international_politics\", \"nationalism\", \"oratory\", \"political_establishment\", \"party_bosses\", \"national_issues\", \"patriotism\", \"American_democracy\", \"grass_roots\", \"real_issues\", \"culture_war\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"society\", \"American_society\", \"culture\", \"societies\", \"larger_society\", \"Japanese_society\", \"Western_culture\", \"free_society\", \"community\", \"American_life\", \"modern_society\", \"work_place\", \"institution\", \"American_family\", \"ideals\", \"democratic_society\", \"social_order\", \"educational_system\", \"larger_community\", \"underclass\", \"modern_world\", \"citizenry\", \"American_culture\", \"civil_society\", \"family_structure\", \"humankind\", \"whole_society\", \"mainstream_society\", \"religion\", \"conformity\", \"own_society\", \"mankind\", \"pluralism\", \"legal_system\", \"profession\", \"beliefs\", \"humanity\", \"orthodoxy\", \"prejudices\", \"Western_civilization\", \"ethos\", \"common_good\", \"the_Catholic_Church\", \"political_system\", \"exploitation\", \"multiculturalism\", \"social_contract\", \"assimilation\", \"tenets\", \"dogma\", \"legal_profession\", \"traditional_values\", \"injustices\", \"value_system\", \"entire_community\", \"values\", \"civilization\", \"family_life\", \"individualism\", \"greater_good\", \"social_change\", \"American_democracy\", \"gay_people\", \"oppression\", \"public_good\", \"norms\", \"morality\", \"Soviet_society\", \"government\", \"religious_faith\", \"social_problems\", \"family_unit\", \"traditions\", \"tyranny\", \"altruism\", \"human_beings\", \"other_cultures\", \"human_race\", \"human_life\", \"social_fabric\", \"freedoms\", \"equality\", \"human_dignity\", \"profit_motive\", \"religious_beliefs\", \"human_nature\", \"mass_media\", \"inequality\", \"feminism\", \"individual_responsibility\", \"social_responsibility\", \"individual\", \"democracy\", \"medical_profession\", \"working_class\", \"personal_responsibility\", \"civilized_society\", \"social_justice\", \"black_Americans\", \"criminal_justice_system\"]\n",
      "[\"money\", \"large_sums\", \"so_much_money\", \"vast_sums\", \"more_money\", \"huge_sums\", \"monies\", \"little_money\", \"too_much_money\", \"tax_money\", \"even_more_money\", \"taxpayer_money\", \"new_money\", \"funds\", \"enough_money\", \"government_money\", \"public_money\", \"extra_money\", \"own_money\", \"public_funds\", \"taxpayer_dollars\", \"millions_of_dollars\", \"as_much_money\", \"tax_dollars\", \"campaign_money\", \"much_money\", \"other_money\", \"tens_of_millions_of_dollars\", \"enormous_sums\", \"hundreds_of_millions_of_dollars\", \"additional_money\", \"how_much_money\", \"Government_money\", \"state_money\", \"private_money\", \"huge_amounts\", \"cash\", \"hundreds_of_thousands_of_dollars\", \"much_more_money\", \"thousands_of_dollars\", \"substantial_amounts\", \"big_money\", \"substantial_sums\", \"extra_cash\", \"windfall\", \"far_more_money\", \"government_funds\", \"other_people's_money\", \"billions_of_dollars\", \"large_amounts\", \"own_funds\", \"more_dollars\", \"campaign_funds\", \"own_pockets\", \"federal_money\", \"large_sum\", \"several_million_dollars\", \"Federal_money\", \"less_money\", \"dollars\", \"city_money\", \"foreign_money\", \"life_savings\", \"more_funds\", \"more_cash\", \"most_money\", \"trust_funds\", \"that_much_money\", \"resources\", \"trust_fund\", \"own_resources\", \"more_than_$100,000\", \"fund\", \"borrowed_money\", \"wealthy_people\", \"such_money\", \"coffers\", \"state_funds\", \"enormous_amounts\", \"favors\", \"additional_funds\", \"substantial_amount\", \"company_money\", \"pension_money\", \"personal_accounts\", \"grant_money\", \"private_funds\", \"more_resources\", \"financial_support\", \"charity\", \"taxpayers\", \"proceeds\", \"scarce_resources\"]\n",
      "[\"culture\", \"American_culture\", \"Western_culture\", \"traditions\", \"ethos\", \"American_life\", \"own_culture\", \"American_society\", \"other_cultures\", \"society\", \"modern_world\", \"cultural_identity\", \"popular_culture\", \"black_culture\", \"mythology\", \"heritage\", \"multiculturalism\", \"everyday_life\", \"national_identity\", \"individualism\", \"societies\", \"assimilation\", \"mores\", \"high_culture\", \"religion\", \"notions\", \"modernity\", \"mass_media\", \"subculture\", \"cultures\", \"consumerism\", \"national_character\", \"ethnic_identity\", \"intellectual_life\", \"civilization\", \"Western_civilization\", \"spirituality\", \"Chinese_culture\", \"Japanese_culture\", \"Indian_culture\", \"myths\", \"larger_world\", \"cultural_heritage\", \"own_history\", \"art_form\", \"Buddhism\", \"modern_life\", \"mass_culture\", \"modern_society\", \"sensibilities\", \"materialism\", \"wider_world\", \"cultural_values\", \"common_language\", \"many_cultures\", \"national_culture\", \"French_culture\", \"self-image\", \"pluralism\", \"discourse\", \"Japanese_society\", \"political_culture\", \"daily_life\", \"myth\", \"orthodoxy\", \"cultural_diversity\", \"ideals\", \"Buddhism\", \"prejudices\", \"melting_pot\", \"cultural_differences\", \"social_structure\", \"long_tradition\", \"separateness\", \"high_art\", \"literature\", \"ideologies\", \"nationalism\", \"dogma\", \"Western_world\", \"modernism\", \"European_culture\", \"human_nature\", \"value_system\", \"modern_age\", \"aesthetics\", \"lifestyles\", \"paradigm\", \"Marxism\", \"art\", \"nature\", \"stereotypes\", \"human_experience\", \"hierarchies\", \"humanism\", \"feminism\", \"meritocracy\", \"social_fabric\", \"natural_world\", \"pop_culture\"]\n",
      "[\"convince\", \"persuade\", \"reassure\", \"assure\", \"prove\", \"convinced\", \"convincing\", \"demonstrate\", \"appease\", \"impress\", \"dissuade\", \"believe\", \"satisfy\", \"assert\", \"placate\", \"motivate\", \"remind\"]\n",
      "[\"discredit\", \"silence\", \"rebut\", \"sabotage\", \"embarrass\", \"intimidate\", \"quash\", \"political_enemies\", \"demonize\", \"tar\", \"suppress\", \"implicate\", \"discrediting\", \"refute\", \"mislead\", \"deceive\", \"assassinate\", \"subvert\", \"witch_hunt\", \"defame\", \"exonerate\", \"overthrow\", \"thwart\", \"dirty_tricks\", \"smear\", \"smear_campaign\", \"distance\", \"blackmail\", \"instigate\", \"political_purposes\", \"muzzle\", \"undermine\", \"political_opponents\", \"disavow\", \"disinformation\", \"humiliate\", \"repudiate\", \"deflect\", \"coerce\", \"blunt\", \"counter\", \"depose\", \"foment\", \"squelch\", \"vindicate\", \"obstruct\", \"buttress\", \"political_gain\", \"rebutting\", \"vendetta\", \"politicize\", \"instigated\", \"assail\", \"vilify\", \"unmask\", \"cover-up\", \"placate\", \"debunk\", \"undercut\", \"whitewash\", \"portray\", \"impugn\", \"appease\", \"oust\", \"dissuade\", \"marginalize\", \"mollify\", \"denounce\", \"goad\", \"purge\", \"plotters\", \"destabilize\", \"accuse\", \"outflank\", \"damaging_information\", \"characterizing\", \"further\", \"political_foes\", \"implicating\", \"entrap\", \"character_assassination\", \"disassociate\", \"disavowed\", \"punish\", \"rebuke\", \"defuse\", \"attack\", \"condoned\", \"political_motives\", \"frustrate\", \"incite\", \"malign\", \"disprove\", \"dissociate\", \"expose\", \"taint\", \"repress\", \"coverup\", \"orchestrating\", \"bully\"]\n",
      "[\"fact\", \"indeed\", \"nevertheless\", \"though\", \"However\", \"actually\", \"however\", \"Yet\", \"clearly\", \"implication\", \"although\", \"because\", \"Unfortunately\", \"Perhaps\", \"nonetheless\", \"only\", \"though\", \"Furthermore\", \"because\", \"Certainly\", \"And\", \"simple_fact\", \"other_words\", \"even\", \"Moreover\", \"past\", \"Thus\", \"obviously\", \"That\", \"simply\", \"necessarily\", \"Nevertheless\", \"Indeed\", \"Though\", \"evidently\", \"merely\", \"plainly\", \"certainly\", \"Although\", \"Yet\", \"unfortunately\", \"belief\", \"moreover\", \"yet\", \"so\", \"While\", \"therefore\", \"Apparently\", \"hardly\"]\n",
      "[\"honest\", \"trustworthy\", \"frank\", \"truthful\", \"candid\", \"objective\", \"respectful\", \"honorable\", \"honestly\", \"intelligent\", \"patient\", \"forthright\", \"decent\", \"naive\", \"sincere\", \"sane\", \"smart\", \"polite\", \"dishonest\", \"selfish\", \"sober\", \"judgmental\", \"stubborn\", \"trusting\", \"stupid\", \"na\\u00efve\", \"brave\", \"rational\", \"courteous\", \"arrogant\", \"careless\", \"considerate\", \"cynical\", \"diligent\", \"informed\", \"articulate\", \"humble\", \"cocky\", \"realistic\", \"courageous\", \"trust\", \"mean\", \"conscientious\", \"tough\", \"thoughtful\", \"credible\", \"smug\", \"think\", \"scrupulous\"]\n",
      "[\"trusted\", \"trust\", \"trustworthy\", \"trusting\", \"deceived\", \"trusts\", \"respected\"]\n"
     ]
    }
   ],
   "source": [
    "lexicon = Empath()\n",
    "categories = [\n",
    "    \"sarcastic\",\n",
    "    \"ironic\",\n",
    "    \"contradict\",\n",
    "    \"mock\",\n",
    "    \"jest\",\n",
    "    \"malicious\",\n",
    "    \"vinidctive\",\n",
    "    \"government\",\n",
    "    \"politics\",\n",
    "    \"society\",\n",
    "    \"money\",\n",
    "    \"culture\",\n",
    "    \"convince\",\n",
    "    \"discredit\",\n",
    "    \"fact\",\n",
    "    \"honest\",\n",
    "    \"trusted\",\n",
    "]\n",
    "\n",
    "for cat in categories:\n",
    "    lexicon.create_category(cat, [cat], model=\"nytimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab6c1b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Info: \n",
      "\n",
      " {'jest': 1.0}\n",
      "\n",
      "Vectorized: \n",
      "\n",
      " 1.0\n",
      "<class 'numpy.float64'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['jest'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"World Champion skier and Olympic gold medal favorite Lindsey Vonn admitted yesterday that the secret to her success is her 'really, really good ski poles.' 'There's no way I would have won 31 World Cup races without these great, great ski poles,' Vonn told reporters during a press conference, noting that without the top-of-the-line ski poles, it would be difficult for her to maintain her balance or change directions during competition. 'I use them a lot because I'm always skiing, and they haven't broken in half or anything. I think they're really expensive too, like over 50 bucks.' Vonn, who said she was unsure if her ski poles were made of graphite or carbon fiber, urged reporters to trust her when she said that 'whatever they're made of is definitely the best.' \"\n",
    "emotion_info = lexicon.analyze(text, categories=[\"jest\"])\n",
    "print(\"Emotion Info: \\n\\n\", emotion_info)\n",
    "\n",
    "dict_vectorizer = DictVectorizer()\n",
    "vec_emotion_info = dict_vectorizer.fit_transform(emotion_info).toarray()[0][0]\n",
    "print(\"\\nVectorized: \\n\\n\", vec_emotion_info)\n",
    "print(type(vec_emotion_info))\n",
    "\n",
    "dict_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "314828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_categories(data):\n",
    "    lexical_categories = []\n",
    "    dict_vectorizer = DictVectorizer()\n",
    "    lexicon = Empath()\n",
    "    for cat in categories:\n",
    "        data[\"Lexicon - \" + cat] = data[\"Text_Clean\"].apply(lambda x: dict_vectorizer\n",
    "                                                        .fit_transform(lexicon.analyze(x, categories=[cat]))\n",
    "                                                        .toarray()[0][0])\n",
    "#     vec_emotion_info = (emotion_info).toarray().flatten()\n",
    "# #     data[\"Lexicon\"] = vec_emotion_info\n",
    "#     lexical_categories.append(vec_emotion_info)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84345c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "      <th>Char_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Capital_Chars_Count</th>\n",
       "      <th>Capital_Words_Count</th>\n",
       "      <th>Blob_Polarity</th>\n",
       "      <th>...</th>\n",
       "      <th>Lexicon - government</th>\n",
       "      <th>Lexicon - politics</th>\n",
       "      <th>Lexicon - society</th>\n",
       "      <th>Lexicon - money</th>\n",
       "      <th>Lexicon - culture</th>\n",
       "      <th>Lexicon - convince</th>\n",
       "      <th>Lexicon - discredit</th>\n",
       "      <th>Lexicon - fact</th>\n",
       "      <th>Lexicon - honest</th>\n",
       "      <th>Lexicon - trusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24561</td>\n",
       "      <td>3</td>\n",
       "      <td>Third Journalist Killed in 3 Months in Turkey ...</td>\n",
       "      <td>third journalist killed 3 month turkey suspect...</td>\n",
       "      <td>[third, journalist, killed, 3, month, turkey, ...</td>\n",
       "      <td>6024</td>\n",
       "      <td>951</td>\n",
       "      <td>235</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.031443</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19348</td>\n",
       "      <td>2</td>\n",
       "      <td>Welfare Leech With 12 Kids Collects More In Be...</td>\n",
       "      <td>welfare leech 12 kid collect benefit make enti...</td>\n",
       "      <td>[welfare, leech, 12, kid, collect, benefit, ma...</td>\n",
       "      <td>2408</td>\n",
       "      <td>434</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35758</td>\n",
       "      <td>3</td>\n",
       "      <td>Study Involving 18,000 People Confirms Acupunc...</td>\n",
       "      <td>study involving 18 000 people confirms acupunc...</td>\n",
       "      <td>[study, involving, 18, 000, people, confirms, ...</td>\n",
       "      <td>2706</td>\n",
       "      <td>398</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6252</td>\n",
       "      <td>1</td>\n",
       "      <td>When Enron founder Kenneth Lay died suddenly, ...</td>\n",
       "      <td>enron founder kenneth lay died suddenly le two...</td>\n",
       "      <td>[enron, founder, kenneth, lay, died, suddenly,...</td>\n",
       "      <td>1956</td>\n",
       "      <td>341</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.176077</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9815</td>\n",
       "      <td>1</td>\n",
       "      <td>Determined to create the definitive visual doc...</td>\n",
       "      <td>determined create definitive visual document p...</td>\n",
       "      <td>[determined, create, definitive, visual, docum...</td>\n",
       "      <td>1064</td>\n",
       "      <td>163</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0.202329</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Label                                               Text  \\\n",
       "0  24561      3  Third Journalist Killed in 3 Months in Turkey ...   \n",
       "1  19348      2  Welfare Leech With 12 Kids Collects More In Be...   \n",
       "2  35758      3  Study Involving 18,000 People Confirms Acupunc...   \n",
       "3   6252      1  When Enron founder Kenneth Lay died suddenly, ...   \n",
       "4   9815      1  Determined to create the definitive visual doc...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  third journalist killed 3 month turkey suspect...   \n",
       "1  welfare leech 12 kid collect benefit make enti...   \n",
       "2  study involving 18 000 people confirms acupunc...   \n",
       "3  enron founder kenneth lay died suddenly le two...   \n",
       "4  determined create definitive visual document p...   \n",
       "\n",
       "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
       "0  [third, journalist, killed, 3, month, turkey, ...        6024         951   \n",
       "1  [welfare, leech, 12, kid, collect, benefit, ma...        2408         434   \n",
       "2  [study, involving, 18, 000, people, confirms, ...        2706         398   \n",
       "3  [enron, founder, kenneth, lay, died, suddenly,...        1956         341   \n",
       "4  [determined, create, definitive, visual, docum...        1064         163   \n",
       "\n",
       "   Capital_Chars_Count  Capital_Words_Count  Blob_Polarity  ...  \\\n",
       "0                  235                   22      -0.031443  ...   \n",
       "1                   66                   10       0.023333  ...   \n",
       "2                   63                    1       0.133428  ...   \n",
       "3                   46                    1       0.176077  ...   \n",
       "4                   23                    2       0.202329  ...   \n",
       "\n",
       "   Lexicon - government Lexicon - politics  Lexicon - society  \\\n",
       "0                  22.0               10.0                2.0   \n",
       "1                  11.0                0.0                2.0   \n",
       "2                   0.0                0.0                0.0   \n",
       "3                   1.0                1.0                0.0   \n",
       "4                   2.0                0.0                0.0   \n",
       "\n",
       "   Lexicon - money  Lexicon - culture  Lexicon - convince  \\\n",
       "0              0.0                1.0                 0.0   \n",
       "1              0.0                0.0                 0.0   \n",
       "2              1.0                0.0                 0.0   \n",
       "3             12.0                1.0                 0.0   \n",
       "4              0.0                0.0                 1.0   \n",
       "\n",
       "   Lexicon - discredit  Lexicon - fact  Lexicon - honest  Lexicon - trusted  \n",
       "0                  2.0             5.0               3.0                1.0  \n",
       "1                  0.0             7.0               3.0                0.0  \n",
       "2                  0.0             3.0               4.0                0.0  \n",
       "3                  0.0             2.0               1.0                0.0  \n",
       "4                  0.0             1.0               0.0                0.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = get_lexical_categories(train) \n",
    "train.head()\n",
    "# train_lexical_categories = get_lexical_categories(train) \n",
    "# train_lexical_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a43eab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "      <th>Char_Count</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>Capital_Chars_Count</th>\n",
       "      <th>Capital_Words_Count</th>\n",
       "      <th>Blob_Polarity</th>\n",
       "      <th>Blob_Subjectivity</th>\n",
       "      <th>...</th>\n",
       "      <th>Lexicon - government</th>\n",
       "      <th>Lexicon - politics</th>\n",
       "      <th>Lexicon - society</th>\n",
       "      <th>Lexicon - money</th>\n",
       "      <th>Lexicon - culture</th>\n",
       "      <th>Lexicon - convince</th>\n",
       "      <th>Lexicon - discredit</th>\n",
       "      <th>Lexicon - fact</th>\n",
       "      <th>Lexicon - honest</th>\n",
       "      <th>Lexicon - trusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>When so many actors seem content to churn out ...</td>\n",
       "      <td>many actor seem content churn performance quic...</td>\n",
       "      <td>[many, actor, seem, content, churn, performanc...</td>\n",
       "      <td>1356</td>\n",
       "      <td>251</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>0.124375</td>\n",
       "      <td>0.509644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In what football insiders are calling an unex...</td>\n",
       "      <td>football insider calling unexpectedly severe p...</td>\n",
       "      <td>[football, insider, calling, unexpectedly, sev...</td>\n",
       "      <td>1173</td>\n",
       "      <td>202</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0.130303</td>\n",
       "      <td>0.543603</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>In a freak accident following Game 3 of the N....</td>\n",
       "      <td>freak accident following game 3 n b final clev...</td>\n",
       "      <td>[freak, accident, following, game, 3, n, b, fi...</td>\n",
       "      <td>979</td>\n",
       "      <td>167</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>North Koreas official news agency announced to...</td>\n",
       "      <td>north korea official news agency announced tod...</td>\n",
       "      <td>[north, korea, official, news, agency, announc...</td>\n",
       "      <td>814</td>\n",
       "      <td>134</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.375054</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The former Alaska Governor Sarah Palin would b...</td>\n",
       "      <td>former alaska governor sarah palin would bring...</td>\n",
       "      <td>[former, alaska, governor, sarah, palin, would...</td>\n",
       "      <td>1120</td>\n",
       "      <td>177</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>0.067614</td>\n",
       "      <td>0.334870</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text  \\\n",
       "0      1  When so many actors seem content to churn out ...   \n",
       "1      1   In what football insiders are calling an unex...   \n",
       "2      1  In a freak accident following Game 3 of the N....   \n",
       "3      1  North Koreas official news agency announced to...   \n",
       "4      1  The former Alaska Governor Sarah Palin would b...   \n",
       "\n",
       "                                          Text_Clean  \\\n",
       "0  many actor seem content churn performance quic...   \n",
       "1  football insider calling unexpectedly severe p...   \n",
       "2  freak accident following game 3 n b final clev...   \n",
       "3  north korea official news agency announced tod...   \n",
       "4  former alaska governor sarah palin would bring...   \n",
       "\n",
       "                                      Text_Tokenized  Char_Count  Word_Count  \\\n",
       "0  [many, actor, seem, content, churn, performanc...        1356         251   \n",
       "1  [football, insider, calling, unexpectedly, sev...        1173         202   \n",
       "2  [freak, accident, following, game, 3, n, b, fi...         979         167   \n",
       "3  [north, korea, official, news, agency, announc...         814         134   \n",
       "4  [former, alaska, governor, sarah, palin, would...        1120         177   \n",
       "\n",
       "   Capital_Chars_Count  Capital_Words_Count  Blob_Polarity  Blob_Subjectivity  \\\n",
       "0                   31                    4       0.124375           0.509644   \n",
       "1                   40                    2       0.130303           0.543603   \n",
       "2                   27                    1      -0.062500           0.426667   \n",
       "3                   28                    2       0.003409           0.375054   \n",
       "4                   36                    4       0.067614           0.334870   \n",
       "\n",
       "   ... Lexicon - government  Lexicon - politics  Lexicon - society  \\\n",
       "0  ...                  0.0                 0.0                0.0   \n",
       "1  ...                  1.0                 1.0                0.0   \n",
       "2  ...                  0.0                 0.0                0.0   \n",
       "3  ...                  5.0                 1.0                0.0   \n",
       "4  ...                  2.0                 7.0                0.0   \n",
       "\n",
       "   Lexicon - money  Lexicon - culture  Lexicon - convince  \\\n",
       "0              1.0                0.0                 0.0   \n",
       "1              0.0                0.0                 1.0   \n",
       "2              2.0                0.0                 0.0   \n",
       "3              0.0                0.0                 0.0   \n",
       "4              0.0                0.0                 0.0   \n",
       "\n",
       "   Lexicon - discredit  Lexicon - fact  Lexicon - honest  Lexicon - trusted  \n",
       "0                  0.0             3.0               0.0                0.0  \n",
       "1                  0.0             1.0               0.0                0.0  \n",
       "2                  0.0             0.0               0.0                0.0  \n",
       "3                  0.0             0.0               1.0                0.0  \n",
       "4                  0.0             0.0               1.0                0.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_lexical_categories(test) \n",
    "test.head()\n",
    "# test_lexical_categories = get_lexical_categories(test) \n",
    "# test_lexical_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4c471ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - sarcastic']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 181 427]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - ironic']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[472  25 244   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 181 427]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - contradict']\n",
      "accuracy:   0.558\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[472  25 244   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - mock']\n",
      "accuracy:   0.558\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[472  25 244   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - jest']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - malicious']\n",
      "accuracy:   0.558\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[472  25 244   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - vinidctive']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - government']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 182 426]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - politics']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - society']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[472  25 244   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 181 427]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - money']\n",
      "accuracy:   0.558\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[472  25 244   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - culture']\n",
      "accuracy:   0.558\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[472  25 244   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - convince']\n",
      "accuracy:   0.558\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[472  25 244   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - discredit']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.95      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[470  25 246   9]\n",
      " [114  44 583   9]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - fact']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 181 427]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - honest']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/rachelng/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blob_Polarity', 'Blob_Subjectivity', 'Lexicon - trusted']\n",
      "accuracy:   0.557\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       1 - Satire       0.63      0.63      0.63       750\n",
      "         2 - Hoax       0.64      0.06      0.11       750\n",
      "   3 - Propaganda       0.42      0.97      0.59       750\n",
      "4 - Reliable News       0.96      0.57      0.71       750\n",
      "\n",
      "         accuracy                           0.56      3000\n",
      "        macro avg       0.66      0.56      0.51      3000\n",
      "     weighted avg       0.66      0.56      0.51      3000\n",
      "\n",
      "confusion matrix:\n",
      "[[471  25 245   9]\n",
      " [115  44 583   8]\n",
      " [ 18   0 729   3]\n",
      " [142   0 180 428]]\n"
     ]
    }
   ],
   "source": [
    "for cat in categories:\n",
    "    features = [\"Blob_Polarity\", \n",
    "            \"Blob_Subjectivity\",]\n",
    "    \n",
    "    features.append(\"Lexicon - \" + cat)\n",
    "\n",
    "    train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "    test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_X = scaler.fit_transform(train_features)\n",
    "    test_X = scaler.fit_transform(test_features)\n",
    "    \n",
    "    print(features)\n",
    "    \n",
    "    nb_classifer = MultinomialNB()\n",
    "    nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "    pred_y = nb_classifer.predict(test_X)\n",
    "    accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "    print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "    print(metrics.classification_report(test_y, pred_y,\n",
    "                                                target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bec236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c494b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb42fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e3758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
