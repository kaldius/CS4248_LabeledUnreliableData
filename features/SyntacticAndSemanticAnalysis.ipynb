{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca3a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/fulltrain.csv\", names=[\"Label\", \"Text\"])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/balancedtest.csv\", names=[\"Label\", \"Text\"])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf36e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e442567",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4fd25",
   "metadata": {},
   "source": [
    "### Random sampling the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b385de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(n=10000, random_state=12).reset_index()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7af096",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f75363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data['Text_Clean'] = data['Text'].apply(text_lower)\n",
    "    data['Text_Clean'] = data['Text_Clean'].apply(text_remove_special_characters)\n",
    "    data['Text_Clean'] = data['Text_Clean'].apply(text_remove_stopwords)\n",
    "    data['Text_Clean'] = data['Text_Clean'].apply(text_lemmatize)\n",
    "    data['Text_Clean_Tokenized'] = data['Text_Clean'].apply(text_tokenize)\n",
    "    data['Text_Tokenized'] = data['Text'].apply(text_tokenize)\n",
    "    return data\n",
    "\n",
    "def text_lemmatize(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_list = text_tokenize(text)\n",
    "    return \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_list])\n",
    "\n",
    "def text_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def text_remove_special_characters(text):\n",
    "    return re.sub('[^a-zA-Z0-9]',' ', text)\n",
    "\n",
    "def text_remove_links(text):\n",
    "    return re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def text_remove_stopwords(text):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    word_list = text_tokenize(text)\n",
    "    return \" \".join([word for word in word_list if word not in stopword_list])\n",
    "\n",
    "def text_tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def undersample_majority_class(data, y_col, y_value):\n",
    "    majority_index = data.index[data[y_col] == y_value].tolist()\n",
    "    random.seed(10)\n",
    "    random_sample = random.sample(majority_index, round(len(majority_index) * 0.5))\n",
    "    return data.drop(random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d0443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocess(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c71fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055dbb66",
   "metadata": {},
   "source": [
    "# Baseline tf-idf NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a476e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "vectorized_train_X = vectorizer.fit_transform(train[\"Text_Clean\"])\n",
    "train_y = train[\"Label\"]\n",
    "\n",
    "vectorized_test_X = vectorizer.transform(test[\"Text_Clean\"])\n",
    "test_y = test[\"Label\"]\n",
    "\n",
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(vectorized_train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(vectorized_test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c4f13",
   "metadata": {},
   "source": [
    "This will be the baseline to which we aim to improve.\n",
    "\n",
    "From the metrics calculated, we see that Reliable news is being predicted with a precision of 100%. This means that all articles with labelled \"Reliable News\" were correctly identifies as \"Reliable News\". However, articles of other lablels scored lower on the metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3f48c",
   "metadata": {},
   "source": [
    "## Syntactic Feature Engineering  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1359c07",
   "metadata": {},
   "source": [
    "#### Number of Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87744b93",
   "metadata": {},
   "source": [
    "#### Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cecc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af16ae",
   "metadata": {},
   "source": [
    "#### Number of Capital Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c262f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_chars(text):\n",
    "    count=0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045fab8",
   "metadata": {},
   "source": [
    "#### Number of Capital Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007a39c",
   "metadata": {},
   "source": [
    "#### Number of Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ca8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_first_person_pronouns(text):\n",
    "    first_person = len(re.findall(r'\\b(I|me|my|mine|we|us|our|ours)\\b', text, flags=re.IGNORECASE))\n",
    "    return first_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_second_person_pronouns(text):\n",
    "    second_person = len(re.findall(r'\\b(you|your|yours)\\b', text, flags=re.IGNORECASE))\n",
    "    return second_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afd2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_third_person_pronouns(text):\n",
    "    third_person = len(re.findall(r'\\b(he|him|his|she|her|hers|it|its|they|them|their|theirs)\\b', text, flags=re.IGNORECASE))\n",
    "    return third_person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f7090",
   "metadata": {},
   "source": [
    "#### Number of Hedging Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hedges(text):\n",
    "    # Load the list of hedging words from a text file\n",
    "    with open('hedging_words.txt', 'r') as f:\n",
    "        hedging_words = [line.strip() for line in f]\n",
    "\n",
    "    # Use NLTK to tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Count the number of hedging words in the text\n",
    "    num_hedges = sum(1 for word in words if word.lower() in hedging_words)\n",
    "\n",
    "    return num_hedges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d1ade",
   "metadata": {},
   "source": [
    "#### Number of Boosting Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47147b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_boosts(text):\n",
    "    # Load the list of hedging words from a text file\n",
    "    with open('boosting_words.txt', 'r') as f:\n",
    "        boosting_words = [line.strip() for line in f]\n",
    "\n",
    "    # Use NLTK to tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Count the number of hedging words in the text\n",
    "    num_boosts = sum(1 for word in words if word.lower() in boosting_words)\n",
    "\n",
    "    return num_boosts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bed5c4",
   "metadata": {},
   "source": [
    "#### Number of Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b525ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_numbers(text):\n",
    "    pattern = r\"\\d{1,3}(,\\d{3})*(\\.\\d+)?\"  # regular expression pattern to match numbers\n",
    "    matches = re.findall(pattern, text)  # find all matches of the pattern in the text\n",
    "    return len(matches)  # return the count of matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fcae3",
   "metadata": {},
   "source": [
    "#### Number of Positive and Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257419a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_positive_words(words):\n",
    "    # Use the SentimentIntensityAnalyzer to get sentiment scores for each word\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    score = sia.polarity_scores(words)\n",
    "    scores_pos = [score['pos'] for word in words]\n",
    "\n",
    "    # Get the total number of positive words\n",
    "    num_pos_words = sum([1 for score in scores_pos if score > 0])\n",
    "\n",
    "    return num_pos_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b329a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_negative_words(words):\n",
    "    # Use the SentimentIntensityAnalyzer to get sentiment scores for each word\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    score = sia.polarity_scores(words)\n",
    "    scores_neg = [score['neg'] for word in words]\n",
    "\n",
    "    # Get the total number of positive words\n",
    "    num_neg_words = sum([1 for score in scores_neg if score > 0])\n",
    "\n",
    "    return num_neg_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5bcc8f",
   "metadata": {},
   "source": [
    "#### Number of Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea0e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_proper_nouns(pos_tags):\n",
    "    num_proper_nouns = sum(1 for word, tag in pos_tags if tag == 'NNP')\n",
    "    return num_proper_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4fb61",
   "metadata": {},
   "source": [
    "#### Number of Conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_conjunctions(pos_tags):\n",
    "    num_conjunctions = sum(1 for word, tag in pos_tags if tag == 'CC')\n",
    "    return num_conjunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0da85d",
   "metadata": {},
   "source": [
    "#### Number of Superlatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4106df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_superlatives(pos_tags):\n",
    "    num_superlatives = sum(1 for word, tag in pos_tags if tag == \"JJS\")\n",
    "    return num_superlatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d76a8",
   "metadata": {},
   "source": [
    "### Testing Syntactic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_syntactic_features(data):\n",
    "    data['Char_Count'] = data[\"Text_Clean\"].apply(count_chars)\n",
    "    data['Word_Count'] = data[\"Text_Clean\"].apply(count_words)\n",
    "    data['Capital_Chars_Count'] = data[\"Text\"].apply(count_capital_chars)\n",
    "    data['Capital_Words_Count'] = data[\"Text\"].apply(count_capital_words)\n",
    "    \n",
    "    data['First_Person_Pronoun_Count'] = data[\"Text\"].apply(count_third_person_pronouns)\n",
    "    data['Second_Person_Pronoun_Count'] = data[\"Text\"].apply(count_third_person_pronouns)\n",
    "    data['Third_Person_Pronoun_Count'] = data[\"Text\"].apply(count_third_person_pronouns)\n",
    "    data['Boost_Count'] = data[\"Text_Clean\"].apply(count_boosts)\n",
    "    data['Number_Count'] = data[\"Text_Clean\"].apply(count_numbers)\n",
    "    \n",
    "    data['Positive_Word_Count'] = data[\"Text_Clean\"].apply(count_positive_words)\n",
    "    data['Negative_Word_Count'] = data[\"Text_Clean\"].apply(count_negative_words)\n",
    "    \n",
    "    data['pos_tags'] = data[\"Text_Tokenized\"].apply(nltk.pos_tag)\n",
    "    data['Proper_Noun_Count'] = data[\"pos_tags\"].apply(count_proper_nouns)\n",
    "    data['Conjunction_Count'] = data[\"pos_tags\"].apply(count_conjunctions)\n",
    "    data['Superlative_Count'] = data[\"pos_tags\"].apply(count_superlatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_syntactic_features(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_syntactic_features(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3bd22",
   "metadata": {},
   "source": [
    "#### Syntactic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"Char_Count\", \n",
    "    \"Word_Count\",\n",
    "    \"Capital_Chars_Count\", \n",
    "    \"Capital_Words_Count\",\n",
    "    \"First_Person_Pronoun_Count\",\n",
    "    \"Second_Person_Pronoun_Count\",\n",
    "    \"Boost_Count\",\n",
    "    \"Number_Count\",\n",
    "    \"Positive_Word_Count\",\n",
    "    \"Negative_Word_Count\",\n",
    "    \"Proper_Noun_Count\",\n",
    "    \"Conjunction_Count\",\n",
    "    \"Superlative_Count\"\n",
    "]\n",
    "\n",
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train[features], train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test[features])\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942cdd4",
   "metadata": {},
   "source": [
    "#### Syntactic Features + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train_X_df = pd.DataFrame(vectorized_train_X.toarray())\n",
    "vectorized_test_X_df = pd.DataFrame(vectorized_test_X.toarray())\n",
    "\n",
    "train_X = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_X = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac551f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c02f19",
   "metadata": {},
   "source": [
    "## Semantic Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6dc893",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00db1b",
   "metadata": {},
   "source": [
    "#### TextBlob Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcba199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_sentiment_analysis(data):\n",
    "    data['Blob_Polarity'] = data['Text_Clean'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    data['Blob_Subjectivity'] = data['Text_Clean'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = textblob_sentiment_analysis(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8dc376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = textblob_sentiment_analysis(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Blob_Polarity\", \n",
    "            \"Blob_Subjectivity\",]\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ab61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5a347",
   "metadata": {},
   "source": [
    "#### Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_analysis(data):\n",
    "    data['Vader_Scores'] = data['Text_Clean'].apply(lambda x: vader.polarity_scores(x))\n",
    "    data['Vader_Negative'] = data['Vader_Scores'].apply(lambda x: x['neg'])\n",
    "    data['Vader_Neutral'] = data['Vader_Scores'].apply(lambda x: x['neu'])\n",
    "    data['Vader_Positive'] = data['Vader_Scores'].apply(lambda x: x['pos'])\n",
    "    data['Vader_Compound'] = data['Vader_Scores'].apply(lambda x: x['compound'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38223cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = vader_sentiment_analysis(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fcb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vader_sentiment_analysis(test)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8559f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Vader_Negative\", \n",
    "            \"Vader_Neutral\",\n",
    "            \"Vader_Positive\",\n",
    "            \"Vader_Compound\"]\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ff614",
   "metadata": {},
   "source": [
    "#### Combining Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6702fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Blob_Polarity\",\n",
    "            \"Blob_Subjectivity\",\n",
    "            \"Vader_Negative\", \n",
    "            \"Vader_Neutral\",\n",
    "            \"Vader_Positive\",\n",
    "            \"Vader_Compound\"]\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829063e7",
   "metadata": {},
   "source": [
    "### Context Incongruity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b9903",
   "metadata": {},
   "source": [
    "#### Opposite Polarity N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c262648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_gram(tokenized,ngram=1):\n",
    "    temp = zip(*[tokenized[i:] for i in range(0,ngram)])\n",
    "    ans = [' '.join(ngram) for ngram in temp]\n",
    "    return ans\n",
    "\n",
    "def get_N_gram_polarities(n_gram):\n",
    "    return list(map(lambda x: vader.polarity_scores(x)[\"compound\"], n_gram))\n",
    "    \n",
    "def count_context_incongruities(tokenized, N):\n",
    "    n_grams = generate_N_gram(tokenized, ngram=N)\n",
    "    n_gram_polarities = get_N_gram_polarities(n_grams)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(n_gram_polarities) - 1):\n",
    "        if n_gram_polarities[i] * n_gram_polarities[i+1] < 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a405b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_incongruities(data, N):\n",
    "    data[\"Context_Incongruity - \" + str(N) + \"-gram\"] = data[\"Text_Tokenized\"].apply(lambda x: count_context_incongruities(x, N))\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b29ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    get_context_incongruities(train, i)\n",
    "    get_context_incongruities(test, i)\n",
    "    \n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9456b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Context_Incongruity - 1-gram\", \n",
    "            \"Context_Incongruity - 2-gram\",\n",
    "            \"Context_Incongruity - 3-gram\",\n",
    "            \"Context_Incongruity - 4-gram\",\n",
    "            \"Context_Incongruity - 5-gram\"]\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9593dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    train_features = pd.concat([vectorized_train_X_df, train[feature]], axis=\"columns\")\n",
    "    test_features = pd.concat([vectorized_test_X_df, test[feature]], axis=\"columns\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_X = scaler.fit_transform(train_features)\n",
    "    test_X = scaler.fit_transform(test_features)\n",
    "    \n",
    "    nb_classifer = MultinomialNB()\n",
    "    nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "    pred_y = nb_classifer.predict(test_X)\n",
    "    accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "    \n",
    "    print(feature)\n",
    "    print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "    print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81f0e3",
   "metadata": {},
   "source": [
    "### Topic Modeling and Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c96084",
   "metadata": {},
   "source": [
    "#### Lexical Categories Analysis using Empath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff242aa",
   "metadata": {},
   "source": [
    "An example of what the code below is executing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db810f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Empath()\n",
    "categories = [\n",
    "    \"sarcastic\",\n",
    "    \"ironic\",\n",
    "    \"contradict\",\n",
    "    \"mock\",\n",
    "    \"jest\",\n",
    "    \"malicious\",\n",
    "    \"vinidctive\",\n",
    "    \"government\",\n",
    "    \"politics\",\n",
    "    \"society\",\n",
    "    \"money\",\n",
    "    \"culture\",\n",
    "    \"convince\",\n",
    "    \"discredit\",\n",
    "    \"fact\",\n",
    "    \"honest\",\n",
    "    \"trusted\",\n",
    "]\n",
    "\n",
    "for cat in categories:\n",
    "    lexicon.create_category(cat, [cat], model=\"nytimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"World Champion skier and Olympic gold medal favorite Lindsey Vonn admitted yesterday that the secret to her success is her 'really, really good ski poles.' 'There's no way I would have won 31 World Cup races without these great, great ski poles,' Vonn told reporters during a press conference, noting that without the top-of-the-line ski poles, it would be difficult for her to maintain her balance or change directions during competition. 'I use them a lot because I'm always skiing, and they haven't broken in half or anything. I think they're really expensive too, like over 50 bucks.' Vonn, who said she was unsure if her ski poles were made of graphite or carbon fiber, urged reporters to trust her when she said that 'whatever they're made of is definitely the best.' \"\n",
    "emotion_info = lexicon.analyze(text, categories=[\"jest\"])\n",
    "print(\"Emotion Info: \\n\\n\", emotion_info)\n",
    "\n",
    "dict_vectorizer = DictVectorizer()\n",
    "vec_emotion_info = dict_vectorizer.fit_transform(emotion_info).toarray()[0][0]\n",
    "print(\"\\nVectorized: \\n\\n\", vec_emotion_info)\n",
    "print(type(vec_emotion_info))\n",
    "\n",
    "dict_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_categories(data):\n",
    "    lexical_categories = []\n",
    "    dict_vectorizer = DictVectorizer()\n",
    "    lexicon = Empath()\n",
    "    for cat in categories:\n",
    "        data[\"Lexicon - \" + cat] = data[\"Text_Clean\"].apply(lambda x: dict_vectorizer\n",
    "                                                        .fit_transform(lexicon.analyze(x, categories=[cat]))\n",
    "                                                        .toarray()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84345c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lexical_categories(train) \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_lexical_categories(test) \n",
    "test.head()\n",
    "# test_lexical_categories = get_lexical_categories(test) \n",
    "# test_lexical_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c471ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categories:\n",
    "    features = [\"Blob_Polarity\", \n",
    "            \"Blob_Subjectivity\",]\n",
    "    \n",
    "    features.append(\"Lexicon - \" + cat)\n",
    "\n",
    "    train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "    test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_X = scaler.fit_transform(train_features)\n",
    "    test_X = scaler.fit_transform(test_features)\n",
    "    \n",
    "    print(features)\n",
    "    \n",
    "    nb_classifer = MultinomialNB()\n",
    "    nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "    pred_y = nb_classifer.predict(test_X)\n",
    "    accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "    print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "    print(metrics.classification_report(test_y, pred_y,\n",
    "                                                target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3f745",
   "metadata": {},
   "source": [
    "## Overall Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb42fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"Char_Count\", \n",
    "    \"Word_Count\",\n",
    "    \"Capital_Chars_Count\", \n",
    "    \"Capital_Words_Count\",\n",
    "    \"First_Person_Pronoun_Count\",\n",
    "    \"Second_Person_Pronoun_Count\",\n",
    "    \"Boost_Count\",\n",
    "    \"Number_Count\",\n",
    "    \"Positive_Word_Count\",\n",
    "    \"Negative_Word_Count\",\n",
    "    \"Proper_Noun_Count\",\n",
    "    \"Conjunction_Count\",\n",
    "    \"Superlative_Count\",\n",
    "    \"Blob_Polarity\", \n",
    "    \"Blob_Subjectivity\",\n",
    "    \"Vader_Negative\", \n",
    "    \"Vader_Neutral\",\n",
    "    \"Vader_Positive\",\n",
    "    \"Vader_Compound\",\n",
    "]\n",
    "\n",
    "for cat in categories:\n",
    "    features.append(\"Lexicon - \" + cat)\n",
    "\n",
    "train_features = pd.concat([vectorized_train_X_df, train[features]], axis=\"columns\")\n",
    "test_features = pd.concat([vectorized_test_X_df, test[features]], axis=\"columns\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_X = scaler.fit_transform(train_features)\n",
    "test_X = scaler.fit_transform(test_features)\n",
    "\n",
    "print(features)\n",
    "\n",
    "nb_classifer = MultinomialNB()\n",
    "nb_classifer.fit(train_X, train_y)\n",
    "\n",
    "pred_y = nb_classifer.predict(test_X)\n",
    "accuracy = metrics.accuracy_score(test_y, pred_y)\n",
    "print(\"accuracy:   %0.3f\" % accuracy)\n",
    "\n",
    "print(metrics.classification_report(test_y, pred_y,\n",
    "                                            target_names=[\"1 - Satire\", \"2 - Hoax\", \"3 - Propaganda\", \"4 - Reliable News\"]))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e3758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c395d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8afad97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b948574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
